{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5576bbd1",
   "metadata": {},
   "source": [
    "# Urban Traffic Anomaly Detection using Variational Autoencoders (VAE)\n",
    "## Using VAE to build a Traffic Anomaly Detector for Early Accident Warnings\n",
    "\n",
    "In this project, we aim to detect anomalies in urban traffic patterns using a probabilistic machine learning approach based on Variational Autoencoders (VAE). \n",
    "\n",
    "We will train the VAE on typical traffic data so that it learns common patterns. Then, we will detect anomalies as deviations from these learned patterns.\n",
    "\n",
    "The dataset used is the METR-LA dataset, which contains traffic speed readings from sensors in Los Angeles collected every 5 minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bbf3e",
   "metadata": {},
   "source": [
    "---\n",
    "## **Outline**\n",
    "### **Goal**\n",
    "\n",
    ">Build a system that spots unusual traffic patterns in real time and warns authorities of possible upcoming accidents.\n",
    "\n",
    "\n",
    "### **How It Works, Simply: “Normal” vs “Strange”**:\n",
    "\n",
    "   * We first teach a model what everyday traffic looks like (no crashes OR really unbalanced dataset with low incidents/traffic-recordings ratio <span style=\"color:red\">{*we suppose our dataset to be really unbalanced, as they do in a paper*}</span> ).\n",
    "   * Then, in operation, the model watches incoming traffic data and flags anything that “doesn’t fit” its learned normal patterns.\n",
    "\n",
    "1. **Data in Chunks**\n",
    "\n",
    "   * preprocessing \n",
    "      * missing data imputation, if any.\n",
    "      * standardization only on training data, or on test data too  (???????????????????????????\n",
    "      * NORMAL TRAFFIC DATA SHOW TIME PATTERNS EVERYTIME EVEN IN REGULAR CONDITIONS, HOW TO ADDRESS THIS THING \\\n",
    "      <span style=\"color:red\">{*should a column with categorical number of week day be added to keep track of regular patterns in time? What about long period patterns, as monthly and yearly patterns??????????????????????????????*}</span> \n",
    "   * We group sensor readings into short time blocks (e.g. one hour of readings) \\\n",
    "   <span style=\"color:red\">{*could the split be made by letting in and letting out a single timestamped row at a time?....for sure it is COMPUTATIONALLY DEMANDINg, any advantage THEN?*}</span> \n",
    "\n",
    "2. **Model Magic (Under the Hood)**\n",
    "\n",
    "   * A compact “autoencoder” network learns to compress each block into a small summary.... \n",
    "      * build the model (which latent sizes, layer widths, KL-weight schedules, etc....)\n",
    "      * maximize ELBO to optimize the parameters (see below theory)\n",
    "      * test (cross validation/others on a validation set (e.g. last 10% of days) )\n",
    "   * <span style=\"color:red\">*TRY DIFFERENT VAE VERSIONS....LOOKING AT LITERATURE RIGHT NOW*</span> \n",
    "\n",
    "\n",
    "3. **Flagging Early Warnings**\n",
    "   * ....and then reconstruct it back. If it reconstructs poorly, that block is probably unusual.\n",
    "   * We check reconstruction error: big errors → possible problem.\n",
    "   * When multiple sensors or blocks light up before a known crash, we call those “early warnings.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0b47b",
   "metadata": {},
   "source": [
    "## **About 'Flagging Early Warnings' (TEMPORARY DRAFT)**\n",
    "\n",
    "Once our model has learned what “normal” traffic looks like, here’s how we catch potential accidents before they happen:\n",
    "\n",
    "1. **Watch the Reconstruction Error**\n",
    "\n",
    "   * **What it is**: For each time block and each sensor, the model tries to rebuild (reconstruct) the recorded speeds. If the rebuilt value is very different from the real one, that’s an error.\n",
    "   * **Why it matters**: A low error means “all good,” a high error means “something strange is going on here.”\n",
    "   * **Compute a Simple Anomaly Score**\n",
    "\n",
    "    * For each sensor in a block, calculate specific sensor’s reconstruction error across the multiple rows of the block. Could be the MSE??????\n",
    "\n",
    "      $$\n",
    "        \\mathrm{Error}_s \\;=\\;\\sum_{t\\in\\text{block}}(\\text{actual}_{t,s}-\\hat{\\text{predicted}}_{t,s})^2\n",
    "      $$\n",
    "\n",
    "\n",
    "2. **Ways to Flag Glitches** \n",
    "    * **2.1 Per-Sensor Alerts (Instant)**\n",
    "\n",
    "      * **What:** As soon as any sensor’s error > its own cutoff (e.g. 99ᵗʰ-percentile), you send a “glitch” alert for that sensor.\n",
    "      * **Why:** Super fast, catches tiny/local faults immediately.\n",
    "      * **Trade-off:** Every alert looks equally urgent and you might get noise.\n",
    "\n",
    "    * **2.2 Add a Quick Severity Score (Prioritized)**\n",
    "\n",
    "      * **Measure:**\n",
    "\n",
    "        1. **MaxErrorRatio** = worst sensor’s error ÷ its normal 95ᵗʰ-percentile error\n",
    "        2. **CountFired** = how many sensors are over their cutoffs\n",
    "        3. **SeverityScore** = MaxErrorRatio × √CountFired\n",
    "\n",
    "      * **Map to Low/Med/High:**\n",
    "\n",
    "        * Low if < 2 • Medium if 2–5 • High if ≥ 5\n",
    "      * **Why:** Highlights big or widespread glitches, cuts noise, still trivial math.\n",
    "    \n",
    "    * **2.3 Set Smart Thresholds**\n",
    "      * Summarize across sensors, e.g.:\n",
    "\n",
    "          $$\n",
    "            \\text{BlockScore} = \\max_s \\; \\text{Error}_{s}\n",
    "          $$\n",
    "      * Intuition: if even one sensor suddenly behaves oddly, the block gets flagged.\n",
    "      * Look at BlockScores over a week of known “normal” days.\n",
    "      * Choose a cutoff (e.g. the 95ᵗʰ percentile) so that only the largest 5% of errors trigger an alert.\n",
    "      * This keeps false alarms low while still catching real issues.\n",
    "      \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "4. **Detecting Early Warnings vs. Aftermath**\n",
    "\n",
    "   * **Early Warning**: If a high BlockScore occurs **before** any reported incident, mark it as a “precursor.”\n",
    "   * **Aftermath**: If it happens **during or immediately after** a crash, label it “consequence.”\n",
    "   * We’ll visualize these on a timeline—warning flags should lead incidents by several minutes if truly predictive.\n",
    "\n",
    "5. **Optional Classifier for Precision**\n",
    "\n",
    "   * Feed the model’s internal summary (its compact “code” for each block) into a tiny decision tree or logistic regression.\n",
    "   * Train it on a handful of labeled examples (blocks known to be “warnings” vs. “consequences”). \\\n",
    "   <span style=\"color:red\">{*just know two datasets needs to be jointed and a categorical variable eventually added. (???). We also have a dataset with coordinates of incidents, and our traffic dataset has references to coordinates too*}</span>\n",
    "   * This helps the system learn subtle patterns that pure error thresholds might miss (e.g. gradual slowdowns vs. sudden stops).\n",
    "\n",
    "6. **(Ideal) Real-Time Alerting Flow**\n",
    "\n",
    "   1. New block arrives every 5 min.\n",
    "   2. Model computes BlockScore on the fly.\n",
    "   3. If BlockScore > threshold → flag an alert.\n",
    "   4. Check classifier (if used) to tag it “precursor” or “aftermath.”\n",
    "   5. Push notification to dashboard and/or SMS/email to operators.\n",
    "\n",
    "\n",
    "This step-by-step approach keeps things transparent—each alert is simply “error too big,” optionally refined by a lightweight classifier, and timed relative to known incidents to tell if it really is an early warning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb969d44",
   "metadata": {},
   "source": [
    "## **Do We Have Enough Data?**\n",
    "\n",
    "* **Learning “normal” traffic**: \\~2–4 weeks of continuous data at 5 min intervals (i.e. 400–800 windows) usually suffices to cover daily and weekly patterns.\n",
    "* **Distinguishing precursors vs. consequences**: Aim for **50–100 past incidents** to reliably calibrate thresholds and/or train the simple classifier.\n",
    "\n",
    "  * If you see \\~1 incident per day, 2–3 months of data will do; if incidents occur \\~1 per week, plan for 1–2 years of history.\n",
    "\n",
    "In short: modeling normal behavior is quick, but confidently tagging early warnings requires a few dozen labeled incidents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd1335",
   "metadata": {},
   "source": [
    "## **3-Week Roadmap (TEMPORARY DRAFT)**\n",
    "\n",
    "| Week  | Focus & Deliverables                                                                                                                                                                                                                                                                                                                |\n",
    "| ----- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **1** | **VAE model** <br>• Preprocessing on our traffic data.<br>• Add day-of-week and hour-of-day so the model knows “rush hour vs. quiet time.” <span style=\"color:red\">{*?????not sure ?????*}</span> <br>• Build a simple model that learns “normal” patterns. <br>• Test it. <br>• Calibrate the “error threshold” so we catch real incidents but avoid too many false alarms.  <span style=\"color:red\">{*How to compute each sensor’s 95ᵗʰ-percentile error?*}</span>                           |                                                                           |\n",
    "| **2** | **Early-Warning Classifier & Demo**<br>• Either build per-sensor alerts AND/OR add SeverityScore formula & pick cutoffs. <br> • or Mark blocks before known accidents as “warnings” and others as “after” or “normal.”<br>• Train a small classifier on the model’s summary outputs to tell “warning vs. aftermath.”<br>• Put together a simple live dashboard that shows sensor map, current alerts, and warning lead times. |\n",
    "| **4** | **(Optional) real time pipeline**<br>• Hook into real-time pipeline (every 5 min)<br>• Dashboard shows sensor flags + severity |\n",
    "\n",
    "### **Ideal goal** (again)\n",
    "\n",
    "> Real time system to flag anomalies and pop out early-warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f747c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Environment Setup**\n",
    "\n",
    "We will use Python 3.8+ and the following key libraries:\n",
    "\n",
    "- `h5py`: to read `.h5` dataset files  \n",
    "- `tables` (PyTables): a dependency needed by `pandas` to handle HDF5 files  \n",
    "- `numpy` and `pandas`: for data manipulation  \n",
    "- `matplotlib`: for visualization  \n",
    "- `torch` (PyTorch): to build and train our VAE model  \n",
    "\n",
    "### Installing required packages\n",
    "\n",
    "You can install them via pip:\n",
    "\n",
    "```bash\n",
    "pip install numpy pandas matplotlib h5py tables torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a3e5",
   "metadata": {},
   "source": [
    "## **Data Description**\n",
    "\n",
    "\n",
    "**METR-LA Dataset**\n",
    "\n",
    "- Contains traffic speed data from 207 sensors in Los Angeles.\n",
    "- Data is recorded every 5 minutes, resulting in 12 records per hour.\n",
    "- The data is stored in an HDF5 file format (`metr-la.h5`), where rows represent timestamps and columns correspond to different sensors.\n",
    "- Additionally, a precomputed sensor graph adjacency matrix is provided (`adj_mx.pkl`) which encodes the spatial relations between sensors.\n",
    "\n",
    "The data shape is approximately (34272, 207), meaning 34,272 time steps and 207 sensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ec621",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We will load the data from the `.h5` file using `h5py`, normalize the data, and create sliding windows of size 12 (1 hour of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34272, 207)\n",
      "<bound method NDFrame.head of                         773869     767541     767542     717447     717446  \\\n",
      "2012-03-01 00:00:00  64.375000  67.625000  67.125000  61.500000  66.875000   \n",
      "2012-03-01 00:05:00  62.666667  68.555556  65.444444  62.444444  64.444444   \n",
      "2012-03-01 00:10:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
      "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "...                        ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  65.000000  65.888889  68.555556  61.666667   0.000000   \n",
      "2012-06-27 23:40:00  61.375000  65.625000  66.500000  62.750000   0.000000   \n",
      "2012-06-27 23:45:00  67.000000  59.666667  69.555556  61.000000   0.000000   \n",
      "2012-06-27 23:50:00  66.750000  62.250000  66.000000  59.625000   0.000000   \n",
      "2012-06-27 23:55:00  65.111111  66.888889  66.777778  61.222222   0.000000   \n",
      "\n",
      "                        717445     773062     767620     737529     717816  \\\n",
      "2012-03-01 00:00:00  68.750000  65.125000  67.125000  59.625000  62.750000   \n",
      "2012-03-01 00:05:00  68.111111  65.000000  65.000000  57.444444  63.333333   \n",
      "2012-03-01 00:10:00  66.250000  64.500000  64.250000  63.875000  65.375000   \n",
      "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "...                        ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  54.555556  62.444444  63.333333  59.222222  65.333333   \n",
      "2012-06-27 23:40:00  50.500000  62.000000  67.000000  65.250000  67.125000   \n",
      "2012-06-27 23:45:00  44.777778  64.222222  63.777778  59.777778  57.666667   \n",
      "2012-06-27 23:50:00  53.000000  64.285714  64.125000  60.875000  66.250000   \n",
      "2012-06-27 23:55:00  49.555556  65.777778  65.111111  63.000000  61.666667   \n",
      "\n",
      "                     ...     772167     769372     774204     769806  \\\n",
      "2012-03-01 00:00:00  ...  45.625000  65.500000  64.500000  66.428571   \n",
      "2012-03-01 00:05:00  ...  50.666667  69.875000  66.666667  58.555556   \n",
      "2012-03-01 00:10:00  ...  44.125000  69.000000  56.500000  59.250000   \n",
      "2012-03-01 00:15:00  ...   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00  ...   0.000000   0.000000   0.000000   0.000000   \n",
      "...                  ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  ...  52.888889  69.000000  65.111111  55.666667   \n",
      "2012-06-27 23:40:00  ...  54.000000  69.250000  60.125000  60.500000   \n",
      "2012-06-27 23:45:00  ...  51.333333  67.888889  64.333333  57.000000   \n",
      "2012-06-27 23:50:00  ...  51.125000  69.375000  61.625000  60.500000   \n",
      "2012-06-27 23:55:00  ...  56.000000  67.444444  64.888889  60.888889   \n",
      "\n",
      "                        717590     717592     717595     772168     718141  \\\n",
      "2012-03-01 00:00:00  66.875000  59.375000  69.000000  59.250000  69.000000   \n",
      "2012-03-01 00:05:00  62.000000  61.111111  64.444444  55.888889  68.444444   \n",
      "2012-03-01 00:10:00  68.125000  62.500000  65.625000  61.375000  69.857143   \n",
      "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "...                        ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  66.333333  62.444444  66.777778  64.888889  69.666667   \n",
      "2012-06-27 23:40:00  67.250000  59.375000  66.000000  61.250000  69.000000   \n",
      "2012-06-27 23:45:00  66.000000  62.666667  68.666667  63.333333  67.444444   \n",
      "2012-06-27 23:50:00  65.625000  66.375000  69.500000  63.000000  67.875000   \n",
      "2012-06-27 23:55:00  64.222222  66.444444  68.444444  63.555556  68.666667   \n",
      "\n",
      "                        769373  \n",
      "2012-03-01 00:00:00  61.875000  \n",
      "2012-03-01 00:05:00  62.875000  \n",
      "2012-03-01 00:10:00  62.000000  \n",
      "2012-03-01 00:15:00   0.000000  \n",
      "2012-03-01 00:20:00   0.000000  \n",
      "...                        ...  \n",
      "2012-06-27 23:35:00  62.333333  \n",
      "2012-06-27 23:40:00  62.000000  \n",
      "2012-06-27 23:45:00  61.222222  \n",
      "2012-06-27 23:50:00  63.500000  \n",
      "2012-06-27 23:55:00  61.777778  \n",
      "\n",
      "[34272 rows x 207 columns]>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ruta al archivo .h5\n",
    "file_path = 'data/metr-la.h5'\n",
    "df = pd.read_hdf(file_path)\n",
    "print(df.shape)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9323f4",
   "metadata": {},
   "source": [
    "#### Data Normalization\n",
    "\n",
    "We use `StandardScaler` from scikit-learn to apply Z-score normalization to the dataset.\n",
    "\n",
    "Each sensor (column) is standardized independently by removing the mean and scaling to unit variance:\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(df.values)  # shape: (34272, 207)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e8288",
   "metadata": {},
   "source": [
    "#### Sliding Windows\n",
    "\n",
    "Sliding windows allow us to structure the time series data as sequences of fixed length, suitable for feeding into our VAE model.\n",
    "\n",
    "The window size of 12 means each sample contains 12 consecutive timesteps (i.e., one hour of data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34260, 12, 207)\n"
     ]
    }
   ],
   "source": [
    "def create_sliding_windows(data, window_size):\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "    return np.array(X)\n",
    "\n",
    "window_size = 12  # 12 pasos = 1 hora\n",
    "X = create_sliding_windows(data_normalized, window_size)  # shape: (34260, 12, 207)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c83a5",
   "metadata": {},
   "source": [
    "#### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a79140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (23982, 12, 207)\n",
      "Validation shape: (5139, 12, 207)\n",
      "Test shape: (5139, 12, 207)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X.shape = (34260, 12, 207)\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "# Definimos proporciones\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculamos índices\n",
    "train_end = int(num_samples * train_ratio)\n",
    "val_end = train_end + int(num_samples * val_ratio)\n",
    "\n",
    "# División\n",
    "X_train = X[:train_end]\n",
    "X_val = X[train_end:val_end]\n",
    "X_test = X[val_end:]\n",
    "\n",
    "print(f'Train shape: {X_train.shape}')\n",
    "print(f'Validation shape: {X_val.shape}')\n",
    "print(f'Test shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c984c",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "\n",
    "A Variational Autoencoder (VAE) is a type of generative model that learns to encode input data into a compressed latent space and then reconstruct it as accurately as possible. Unlike traditional autoencoders, VAEs add a probabilistic framework that makes the latent space continuous and meaningful, which is useful for tasks like anomaly detection.\n",
    "\n",
    "### Main Components\n",
    "\n",
    "#### 1. **Encoder**\n",
    "The encoder network takes an input (e.g., a time window of traffic data) and maps it to a latent distribution. Instead of directly outputting a single latent vector, the encoder outputs:\n",
    "\n",
    "- A mean vector $\\mu$\n",
    "- A standard deviation (or log variance) vector $\\log(\\sigma^2)$\n",
    "\n",
    "These define a multivariate normal distribution from which we sample the latent variable $z$.\n",
    "\n",
    "#### 2. **Latent Sampling**\n",
    "To allow backpropagation through the sampling process, VAEs use the **reparameterization trick**:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "This makes the sampling step differentiable.\n",
    "\n",
    "#### 3. **Decoder**\n",
    "The decoder takes the sampled latent variable $z$ and tries to reconstruct the original input data. The goal is to learn a meaningful latent space that can generate realistic reconstructions.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The VAE optimizes two components in the loss:\n",
    "\n",
    "1. **Reconstruction Loss** (e.g., Mean Squared Error):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{recon}} = ||x - \\hat{x}||^2\n",
    "$$\n",
    "\n",
    "This penalizes the model when its reconstruction $\\hat{x}$ is far from the original input $x$.\n",
    "\n",
    "> **Why use MSE as reconstruction loss?**\n",
    ">\n",
    "> Theoretically, we want to maximize the log-likelihood of the reconstruction, i.e., $\\log p(x|z)$. If we assume that the decoder’s output is Gaussian with fixed variance (i.e., $p(x|z) = \\mathcal{N}(\\hat{x}, \\sigma^2 I)$), then:\n",
    ">\n",
    "> $$\n",
    "> \\log p(x|z) = -\\frac{1}{2\\sigma^2} ||x - \\hat{x}||^2 + \\text{const}\n",
    "> $$\n",
    ">\n",
    "> So minimizing the squared error (MSE) is equivalent to maximizing the likelihood. That’s why MSE acts as a **proxy** for the negative log-likelihood in practice.\n",
    "\n",
    "\n",
    "2. **Kullback-Leibler (KL) Divergence**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}} = D_{\\text{KL}}(q(z|x) || p(z))\n",
    "$$\n",
    "\n",
    "This measures how much the learned latent distribution $q(z|x)$ (given by the encoder) deviates from the standard normal prior $p(z) = \\mathcal{N}(0, I)$.\n",
    "\n",
    "> **In practice**, the encoder outputs $\\mu$ and $\\log(\\sigma^2)$, and the KL divergence between the approximate posterior and the prior can be computed in closed form:\n",
    ">\n",
    "> $$\n",
    "> \\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum \\left(1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2\\right)\n",
    "> $$\n",
    ">\n",
    "> This encourages the model to keep the latent representations close to a standard normal distribution.\n",
    "\n",
    "\n",
    "### Total Loss\n",
    "\n",
    "The total loss combines both terms:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "where $\\beta$ is a hyperparameter (commonly set to 1) that balances reconstruction accuracy and regularization. \n",
    "\n",
    "> **Important:** In theory, we want to **maximize** the Evidence Lower Bound (ELBO), which is:\n",
    ">\n",
    "> $$\n",
    "> \\text{ELBO}(x) = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{\\text{KL}}(q(z|x) || p(z))\n",
    "> $$\n",
    ">\n",
    "> But in practice, optimization libraries **minimize** functions. So minimizing the total loss is equivalent to **maximizing the ELBO**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3f953",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
