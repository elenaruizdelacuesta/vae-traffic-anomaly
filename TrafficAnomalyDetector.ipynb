{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5576bbd1",
   "metadata": {},
   "source": [
    "# Urban Traffic Anomaly Detection using Variational Autoencoders (VAE)\n",
    "\n",
    "In this project, we aim to detect anomalies in urban traffic patterns using a probabilistic machine learning approach based on Variational Autoencoders (VAE). \n",
    "\n",
    "We will train the VAE on typical traffic data so that it learns common patterns. Then, we will detect anomalies as deviations from these learned patterns.\n",
    "\n",
    "The dataset used is the METR-LA dataset, which contains traffic speed readings from sensors in Los Angeles collected every 5 minutes, and {... dataset on road accidents}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a3e5",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "\n",
    "**METR-LA Dataset**\n",
    "\n",
    "- Contains traffic speed data from 207 sensors in Los Angeles.\n",
    "- Data is recorded every 5 minutes, resulting in 12 records per hour.\n",
    "- The data is stored in an HDF5 file format (`metr-la.h5`), where rows represent timestamps and columns correspond to different sensors.\n",
    "- Additionally, a precomputed sensor graph adjacency matrix is provided (`adj_mx.pkl`) which encodes the spatial relations between sensors.\n",
    "\n",
    "The data shape is approximately (34272, 207), meaning 34,272 time steps and 207 sensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81aff0e",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We will use Python 3.8+ and the following key libraries:\n",
    "\n",
    "- `h5py`: to read `.h5` dataset files  \n",
    "- `tables` (PyTables): a dependency needed by `pandas` to handle HDF5 files  \n",
    "- `numpy` and `pandas`: for data manipulation  \n",
    "- `matplotlib`: for visualization  \n",
    "- `torch` (PyTorch): to build and train our VAE model  \n",
    "\n",
    "### Installing required packages\n",
    "\n",
    "You can install them via pip:\n",
    "\n",
    "```bash\n",
    "pip install numpy pandas matplotlib h5py tables torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ec621",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We will load the data from the `.h5` file using `h5py`, normalize the data, and create sliding windows of size 12 (1 hour of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ruta al archivo .h5\n",
    "file_path = 'data/metr-la.h5'\n",
    "df = pd.read_hdf(file_path)\n",
    "print(df.shape)\n",
    "print(df.head)\n",
    "print(df.tail)\n",
    "\n",
    "\n",
    "## PRELIMINARY INVESTIGATION: \n",
    "# plotting 5 plots , one for each time series for RANDOMLY PICKED SENSORS/specific_sensors(option), just for march 2012\n",
    "np.random.seed(0)\n",
    "def plot_random_sensors(df, num_sensors=5, sens=None):\n",
    "    sensors = np.random.choice(df.columns, num_sensors, replace=False)\n",
    "    if sens is not None:\n",
    "        # Filter the sensors based on the provided list\n",
    "        sensors = [s for s in sens if s in df.columns]\n",
    "    df_subset = df[sensors].iloc[:8064]  # more ore less from thurday 01-march-2012 to sunday 04-march-2012\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, sensor in enumerate(sensors):\n",
    "        plt.subplot(num_sensors, 1, i + 1)\n",
    "        plt.plot(df_subset.index, df_subset[sensor], label=sensor)\n",
    "        plt.title(f'Sensor: {sensor}')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return sensors\n",
    "\n",
    "\n",
    "sensors = plot_random_sensors(df, num_sensors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd0527",
   "metadata": {},
   "source": [
    "**Time Features as Conditional Inputs**\n",
    "\n",
    "Urban traffic patterns are highly dependent on temporal context. For example:\n",
    "- Traffic usually peaks during rush hours (e.g., 8 AM and 5 PM).\n",
    "- Weekends typically have different patterns than weekdays.\n",
    "- There are seasonal effects tied to months or holidays.\n",
    "\n",
    "To help our model capture these patterns, we extract cyclical time features from the timestamps:\n",
    "- Hour of day: mapped to sine and cosine to capture the 24-hour periodicity.\n",
    "- Day of week: similarly encoded to capture weekly cycles.\n",
    "- Month of year: encoded to capture seasonal trends.\n",
    "\n",
    "Using both sine and cosine components allows the model to learn the cyclical nature of time (e.g., 23:00 and 0:00 are close in time).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e983b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index)\n",
    "# Create a new DataFrame to store time-based features\n",
    "df_time = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Hour of the day (0-23), encoded using sine and cosine to preserve cyclic nature\n",
    "df_time[\"hour_sin\"] = np.sin(2 * np.pi * df.index.hour / 24)\n",
    "df_time[\"hour_cos\"] = np.cos(2 * np.pi * df.index.hour / 24)\n",
    "\n",
    "# Day of the week (0=Monday, 6=Sunday), also cyclic\n",
    "df_time[\"dow_sin\"] = np.sin(2 * np.pi * df.index.dayofweek / 7)\n",
    "df_time[\"dow_cos\"] = np.cos(2 * np.pi * df.index.dayofweek / 7)\n",
    "\n",
    "# Month of the year (1-12), encoded similarly\n",
    "df_time[\"month_sin\"] = np.sin(2 * np.pi * df.index.month / 12)\n",
    "df_time[\"month_cos\"] = np.cos(2 * np.pi * df.index.month / 12)\n",
    "\n",
    "print(df_time.head())\n",
    "\n",
    "# Plot one day's worth of hour_sin values (12*24 = 288 samples per day)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df_time[\"hour_sin\"][:288])  # 1 dÃ­a si son datos cada 5 minutos\n",
    "plt.title(\"hour_sin over 24 hours\")\n",
    "plt.xlabel(\"5-minute intervals\")\n",
    "plt.ylabel(\"Sine value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9323f4",
   "metadata": {},
   "source": [
    "**Data Normalization**\n",
    "\n",
    "Machine learning models, especially neural networks, tend to perform better when the input features are normalized. Here, we use `StandardScaler` from scikit-learn to apply Z-score normalization to the dataset:\n",
    "\n",
    "- x: the main dataset with traffic speeds from sensors\n",
    "- c: the time condition variables we computed earlier (e.g., hour_sin, dow_cos, etc.)\n",
    "\n",
    "The formula applied is: \n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "- ðœ‡ is the mean\n",
    "- ðœŽ is the standard deviation\n",
    "\n",
    "Each sensor column in x and each time feature in c is scaled independently using this method.\n",
    "\n",
    "We also split the data into training and test sets to ensure that the scaler is fit only on training data, avoiding any data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = df.values.astype(np.float32)\n",
    "c = df_time.values.astype(np.float32)\n",
    "\n",
    "# Split into training and test sets (no shuffling, to preserve temporal order)\n",
    "x_train, x_test, c_train, c_test = train_test_split(x, c, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Standardize the input traffic data (x)\n",
    "scaler_x = StandardScaler()\n",
    "x_train = scaler_x.fit_transform(x_train) # Fit on training data\n",
    "x_test = scaler_x.transform(x_test) # # Transform test data using the same parameters\n",
    "\n",
    "# Standardize the conditional time features (c)\n",
    "scaler_c = StandardScaler()\n",
    "c_train = scaler_c.fit_transform(c_train)\n",
    "c_test = scaler_c.transform(c_test)\n",
    "\n",
    "print(\"length of x_test:\",len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97393879",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "\n",
    "A **Variational Autoencoder** (VAE) is a type of generative model that learns a continuous, probabilistic latent representation of the data. In our case, the VAE will take a window of traffic sensor readings as input, compress it into a low-dimensional latent vector, and then attempt to reconstruct the original readings. By training the VAE on â€œnormalâ€ traffic patterns, it learns which patterns are likely under the data distribution. At inference time, large reconstruction errors can be flagged as anomalies.\n",
    "\n",
    "### Main Components\n",
    "\n",
    "#### 1. **Encoder**\n",
    "The encoder network takes an input (e.g., a time window of traffic data) and maps it to a latent distribution. Instead of directly outputting a single latent vector, the encoder outputs:\n",
    "\n",
    "- A mean vector $\\mu$\n",
    "- A standard deviation (or log variance) vector $\\log(\\sigma^2)$\n",
    "\n",
    "These define a multivariate normal distribution from which we sample the latent variable $z$.\n",
    "\n",
    "#### 2. **Latent Sampling**\n",
    "To allow backpropagation through the sampling process, VAEs use the **reparameterization trick**:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "This makes the sampling step differentiable.\n",
    "\n",
    "#### 3. **Decoder**\n",
    "The decoder takes the sampled latent variable $z$ and tries to reconstruct the original input data. The goal is to learn a meaningful latent space that can generate realistic reconstructions.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The VAE loss has two terms that together form the **Evidence Lower Bound (ELBO)**. We optimize (i.e. minimize) the negative ELBO.\n",
    "\n",
    "1. **Reconstruction Loss** (e.g., Mean Squared Error):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{recon}} = ||x - \\hat{x}||^2\n",
    "$$\n",
    "\n",
    "This penalizes the model when its reconstruction $\\hat{x}$ is far from the original input $x$.\n",
    "\n",
    "> **Why use MSE as reconstruction loss?**\n",
    ">\n",
    "> Theoretically, we want to maximize the log-likelihood of the reconstruction, i.e., $\\log p(x|z)$. If we assume that the decoderâ€™s output is Gaussian with fixed variance (i.e., $p(x|z) = \\mathcal{N}(\\hat{x}, \\sigma^2 I)$), then:\n",
    ">\n",
    "> $$\n",
    "> \\log p(x|z) = -\\frac{1}{2\\sigma^2} ||x - \\hat{x}||^2 + \\text{const}\n",
    "> $$\n",
    ">\n",
    "> So minimizing the squared error (MSE) is equivalent to maximizing the likelihood. Thatâ€™s why MSE acts as a **proxy** for the negative log-likelihood in practice.\n",
    "\n",
    "\n",
    "2. **Kullback-Leibler (KL) Divergence**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}} = D_{\\text{KL}}(q(z|x) || p(z))\n",
    "$$\n",
    "\n",
    "This measures how much the learned latent distribution $q(z|x)$ (given by the encoder) deviates from the standard normal prior $p(z) = \\mathcal{N}(0, I)$.\n",
    "\n",
    "> **In practice**, the encoder outputs $\\mu$ and $\\log(\\sigma^2)$, and the KL divergence between the approximate posterior and the prior can be computed in closed form:\n",
    ">\n",
    "> $$\n",
    "> \\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum \\left(1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2\\right)\n",
    "> $$\n",
    ">\n",
    "> This encourages the model to keep the latent representations close to a standard normal distribution.\n",
    "\n",
    "\n",
    "### Total Loss\n",
    "\n",
    "The total loss combines both terms:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "where $\\beta$ is a hyperparameter (commonly set to 1) that balances reconstruction accuracy and regularization. \n",
    "\n",
    "> **Important:** In theory, we want to **maximize** the Evidence Lower Bound (ELBO), which is:\n",
    ">\n",
    "> $$\n",
    "> \\text{ELBO}(x) = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{\\text{KL}}(q(z|x) || p(z))\n",
    "> $$\n",
    ">\n",
    "> But in practice, optimization libraries **minimize** functions. So minimizing the total loss is equivalent to **maximizing the ELBO**.\n",
    "\n",
    "---\n",
    "\n",
    "## Static Conditional Variational Autoencoder (CVAE)\n",
    "\n",
    "A **Conditional VAE (CVAE)** extends the VAE framework by conditioning both the encoder and decoder on extra context `c`. In our traffic anomaly project, `c` represents **timeâ€related features** (e.g., `hour_sin`, `hour_cos`, `dow_sin`, etc.). By injecting these features, the model can learn how normal traffic patterns change depending on the time of day, day of week, or month.\n",
    "\n",
    "> **Why â€œstaticâ€ CVAE?**  \n",
    "> We call it â€œstaticâ€ because each training example $(x_t, c_t)$ is treated as independent; we do **not** explicitly model temporal sequences (no RNNs or temporal convolutions). The CVAE simply learns a mapping :\n",
    ">\n",
    ">$$\n",
    ">(x_t, c_t) \\;\\;\\longrightarrow\\;\\; z \\;\\;\\longrightarrow\\;\\; (\\,\\hat{x}_t \\mid z, c_t).\n",
    ">$$\n",
    ">\n",
    "> Each pair $(x_t, c_t)$ is processed independently, ignoring direct timeâ€series dependencies beyond what is encoded in $c_t$.\n",
    "\n",
    "### Main Components\n",
    "\n",
    "- **Input (`x`)**: The data to be reconstructed (e.g., sensor readings).\n",
    "- **Condition (`c`)**: Extra information that provides context (e.g., timestamp, holiday flag).\n",
    "- **Encoder**: Maps `x` and `c` into a latent distribution `z ~ q(z|x, c)`.\n",
    "- **Latent space (`z`)**: A compressed, learned representation of the input data.\n",
    "- **Decoder**: Uses `z` and `c` to reconstruct the input: `xÌ‚ = p(x|z, c)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcbb96",
   "metadata": {},
   "source": [
    "**Custom Dataset: `TrafficDataset`**\n",
    "\n",
    "We define a custom `TrafficDataset` to prepare our data in a format compatible with PyTorch's `DataLoader`.\n",
    "\n",
    "This class:\n",
    "- Converts the input features (`x`) and conditions (`c`) into tensors.\n",
    "- Allows easy indexing and batching of the data.\n",
    "- Returns tuples `(x, c)` so both the data and its condition can be used during model training.\n",
    "\n",
    "Using a custom dataset class ensures full control over how data is structured and accessed during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7305ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, x, c):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.c = torch.tensor(c, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single sample (x_t, c_t) given an index\n",
    "        return self.x[idx], self.c[idx]\n",
    "    \n",
    "# Create dataset instances for training and test sets\n",
    "train_dataset = TrafficDataset(x_train, c_train)\n",
    "test_dataset = TrafficDataset(x_test, c_test)\n",
    "\n",
    "# Create DataLoaders to iterate through data in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  #NOT WORKING WITH SHUFFLE TRUE, PLOTS ARE A MESS\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de29f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CVAE base (sin LSTM por ahora) estÃ¡tico ---\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, latent_dim=16):\n",
    "        \"\"\"\n",
    "        input_dim  : dimensionality of the input vector x (e.g., number of sensors)\n",
    "        cond_dim   : dimensionality of the condition vector c (e.g., number of time features)\n",
    "        latent_dim : size of the latent space (z)\n",
    "        \"\"\"\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        # We will concatenate x and c into a single vector of size (input_dim + cond_dim).\n",
    "        # Then we pass through two hidden layers: 128 -> 64.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, 128), # First dense layer\n",
    "            nn.ReLU(),                            # Non-linearity\n",
    "            nn.Linear(128, 64),                   # Second dense layer\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # After the two layers, we map the 64-dimensional hidden state to:\n",
    "        #   1) mu (mean) of size latent_dim\n",
    "        #   2) logvar (log variance) of size latent_dim\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        # The decoder takes a latent sample z (size latent_dim) concatenated with the condition c (size cond_dim),\n",
    "        # forming a vector of size (latent_dim + cond_dim). Then we map:\n",
    "        #   latent_dim + cond_dim -> 64 -> 128 -> input_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim) # Output is same size as original x\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat([x, c], dim=1))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        return self.decoder(torch.cat([z, c], dim=1))\n",
    "\n",
    "    def forward(self, x, c):  \n",
    "        \"\"\"\n",
    "        Full forward pass:\n",
    "        1) Encode (x, c) to obtain mu and logvar.\n",
    "        2) Sample z using the reparameterization trick.\n",
    "        3) Decode (z, c) to get reconstructed x.\n",
    "        Returns:\n",
    "            x_recon : reconstructed input\n",
    "            mu      : mean of latent distribution\n",
    "            logvar  : log-variance of latent distribution\n",
    "        \"\"\"\n",
    "        # 1) Encode input + condition â†’ (mu, logvar)\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        # 2) Sample z using reparameterization\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # 3) Decode the sampled z (with c) â†’ reconstruction\n",
    "        x_recon = self.decode(z, c)\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.MSELoss()(recon_x, x)\n",
    "    kl_loss = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Input dimensions from the training data:\n",
    "input_dim = x_train.shape[1]\n",
    "cond_dim = c_train.shape[1]\n",
    "\n",
    "latent_dim = 16\n",
    " \n",
    "model = CVAE(input_dim, cond_dim, latent_dim).to(device)\n",
    "\n",
    "# Create an Adam optimizer to update all model parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_recon, total_kl = 0, 0, 0\n",
    "\n",
    "    # Iterate over the training data in mini-batches provided by train_loader\n",
    "    for x_batch, c_batch in train_loader:\n",
    "        x_batch, c_batch = x_batch.to(device), c_batch.to(device)\n",
    "\n",
    "        # Zero out all accumulated gradients before computing new ones\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the reconstruction, plus mu and logvar from the encoder\n",
    "        x_recon, mu, logvar = model(x_batch, c_batch)\n",
    "\n",
    "        # CVAE loss:\n",
    "        loss, recon_loss, kl_loss = loss_function(x_recon, x_batch, mu, logvar)\n",
    "        \n",
    "        # Backward pass: compute gradients of loss w.r.t. model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters using the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the summed losses across all examples in this batch\n",
    "        total_loss += loss.item() * x_batch.size(0)\n",
    "        total_recon += recon_loss.item() * x_batch.size(0)\n",
    "        total_kl += kl_loss.item() * x_batch.size(0)\n",
    "    \n",
    "    # After finishing all batches in this epoch, compute the average loss per example\n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    avg_loss  = total_loss  / dataset_size\n",
    "    avg_recon = total_recon / dataset_size\n",
    "    avg_kl    = total_kl    / dataset_size\n",
    "\n",
    "    # Print out the epoch summary: average total loss, reconstruction loss, and KL loss\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Loss: {avg_loss:.4f} | \"\n",
    "          f\"Recon: {avg_recon:.4f} | \"\n",
    "          f\"KL: {avg_kl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct all velocities from the test set\n",
    "reconstructed = []\n",
    "originals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, c_batch in test_loader:\n",
    "        x_batch, c_batch = x_batch.to(device), c_batch.to(device)\n",
    "        x_recon, _, _ = model(x_batch, c_batch)\n",
    "        reconstructed.append(x_recon.cpu())\n",
    "        originals.append(x_batch.cpu())\n",
    "\n",
    "x_recon_all = torch.cat(reconstructed, dim=0)  # [6854, 207]\n",
    "x_orig_all = torch.cat(originals, dim=0)       # [6854, 207]\n",
    "print(x_recon_all.shape)\n",
    "print(x_orig_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21571af3",
   "metadata": {},
   "source": [
    "*Note on previous section: model.eval() is missing before 'with torch.no_grad(); how's the model being used in this code snippet?* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263f15c",
   "metadata": {},
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0b345",
   "metadata": {},
   "source": [
    "#### 1. Reconstruction of Speeds for a Single Sample\n",
    "\n",
    "This plot compares the **real** sensor speeds against the **reconstructed** speeds generated by the CVAE model for a specific example (sample index 42).\n",
    "\n",
    "- **Purpose:**  \n",
    "  To visually assess how well the model reproduces the original traffic speed pattern for that particular sample.\n",
    "\n",
    "- **What to look for:**  \n",
    "  - If the two lines closely overlap, the model accurately reconstructs the traffic speeds.  \n",
    "  - Large deviations indicate the model struggles to capture some sensor readings correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ccb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot original vs reconstructed velocities for the selected sample\n",
    "index_sample = 42  # Select a sample index to visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x_orig_all[index_sample], label=\"Real\", alpha=0.7)\n",
    "plt.plot(x_recon_all[index_sample], label=\"Reconstructed\", alpha=0.7)\n",
    "plt.title(f\"Velocity Reconstruction - Sample {index_sample}\")\n",
    "plt.xlabel(\"Sensor\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f8445",
   "metadata": {},
   "source": [
    "#### 2. Reconstruction Error (MSE) by Sensor\n",
    "\n",
    "This bar chart shows the mean squared error (MSE) between the reconstructed and original sensor readings, averaged over the first 12 samples.\n",
    "\n",
    "- **Purpose:**  \n",
    "  To identify which sensors have higher reconstruction errors, signaling areas where the model performance is weaker.\n",
    "\n",
    "- **What to look for:**  \n",
    "  - Sensors with higher MSE values could indicate complex traffic patterns or noisy data.  \n",
    "  - These insights can help:  \n",
    "    - Focus data collection efforts on problematic sensors.  \n",
    "    - Adjust or enhance the model to better capture challenging areas.  \n",
    "    - Check for potential sensor measurement errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Mean Squared Error (MSE) per sensor over the first 12 samples (a small window)\n",
    "mse_by_sensor = torch.mean((x_recon_all[0:12] - x_orig_all[0:12])**2, dim=0)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(207), mse_by_sensor)\n",
    "plt.title(\"Reconstruction Error by Sensor\")\n",
    "plt.xlabel(\"Sensor\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "print(mse_by_sensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "# Compute MSE for each sample and sensor (shape: [num_samples, num_sensors])\n",
    "mse_matrix = (x_orig_all - x_recon_all) ** 2 \n",
    "print(mse_matrix.shape)\n",
    "\n",
    "# Calculate anomaly detection thresholds per sensor\n",
    "# Option 1: Mean + 3*Std deviation (commented out)\n",
    "# mean_mse = mse_matrix.mean(dim=0)\n",
    "# std_mse = mse_matrix.std(dim=0)\n",
    "# thresholds = mean_mse + 3 * std_mse  # threshold per sensor\n",
    "\n",
    "# Option 2: Use percentile threshold (e.g., 99.5th percentile)\n",
    "thresholds = torch.quantile(mse_matrix, 0.995, dim=0)\n",
    "\n",
    "\n",
    "# Detect anomalies: boolean matrix where True means anomaly for that sensor in that sample\n",
    "anomalies = mse_matrix > thresholds\n",
    "# Count how many sensors are anomalous in each sample\n",
    "anomalous_counts = anomalies.sum(dim=1) \n",
    "\n",
    "print(\"Start of test set:\", df.index[-len(x_test)])\n",
    "print(\"End of test set:\", df.index[-1])\n",
    "test_timestamps = df.index[-len(x_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get indices of top 10 samples with most anomalies\n",
    "top_indices = torch.topk(anomalous_counts, 10).indices\n",
    "\n",
    "print(\"Top timestamps with the most anomalies:\")\n",
    "for idx in top_indices:\n",
    "    ts = test_timestamps[idx.item()] # get timestamp for the sample\n",
    "    count = anomalous_counts[idx].item() # get number of anomalous sensors in that sample\n",
    "    print(f\"Index: {idx.item()} â†’ Date: {ts} â†’ Anomalous Sensors: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88285b94",
   "metadata": {},
   "source": [
    "#### 3. Number of Anomalous Sensors Over Time\n",
    "\n",
    "This plot displays the number of sensors detected as anomalous for each timestamp in the test dataset. The x-axis shows the dates, and the y-axis indicates how many sensors were flagged as anomalies at each point in time.\n",
    "\n",
    "Red dashed vertical lines mark important real-world events, such as a parade or a music festival. These event markers help us see if increases in anomalies correspond with these known occurrences.\n",
    "\n",
    "This comparison helps us understand whether unusual sensor behavior is related to specific events or caused by other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71428d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Plot number of anomalous sensors over time (timestamps)\n",
    "ax.plot(test_timestamps, anomalous_counts, label='Anomalous Sensors')\n",
    "\n",
    "# Format X axis to show dates nicely\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))  \n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b'))\n",
    "\n",
    "# Define some important event dates to mark on the plot\n",
    "event_dates = {\n",
    "    'LA Kings Parade': pd.to_datetime('2012-06-14 09:55:00'),\n",
    "    'Make Music LA': pd.to_datetime('2012-06-21 09:10:00'),\n",
    "    'Unexplained Anomaly': pd.to_datetime('2012-06-07 07:35:00')\n",
    "}\n",
    "\n",
    "for label, date in event_dates.items():\n",
    "    ax.axvline(date, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.text(date, max(anomalous_counts), label,\n",
    "            rotation=0, verticalalignment='center', fontsize=8)\n",
    "\n",
    "plt.title(\"Number of Anomalous Sensors per Timestamp\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Anomalous Sensors\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ddd32",
   "metadata": {},
   "source": [
    "On 7 June 2012, no significant events occurred!!! Let us illustrate this to try to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b613fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los timestamps para el 7 de junio 2012\n",
    "mask_7jun = (test_timestamps >= '2012-06-07') & (test_timestamps < '2012-06-08')\n",
    "\n",
    "# Filtrar anomalous_counts para ese dÃ­a\n",
    "anom_7jun = anomalous_counts[mask_7jun]\n",
    "\n",
    "# Filtrar timestamps para ese dÃ­a (para eje x)\n",
    "dates_7jun = test_timestamps[mask_7jun]\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(dates_7jun, anom_7jun, label=\"AnomalÃ­as 7-Jun-2012\")\n",
    "plt.axvline(pd.Timestamp(\"2012-06-07 07:35\"), linestyle=\"--\", label=\"Pico\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d9c5c",
   "metadata": {},
   "source": [
    "#### 4.Statistical Analysis of Anomalies on Normal vs. Special/Holiday Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff777f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# Create a DataFrame with timestamps and anomaly counts\n",
    "df_anom = pd.DataFrame({\n",
    "    'timestamp': test_timestamps,\n",
    "    'anomaly_count': anomalous_counts # Number of anomalous sensors per sample\n",
    "})\n",
    "df_anom['date'] = df_anom['timestamp'].dt.date\n",
    "\n",
    "# Generate US federal holidays within the dataset date range\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start=df_anom['date'].min(),\n",
    "                        end=df_anom['date'].max()).date\n",
    "\n",
    "# 3. Add known special event dates\n",
    "special_event_dates = pd.to_datetime([\n",
    "    '2012-06-14',  # LA Kings Victory Parade\n",
    "    '2012-06-21',  # Make Music LA\n",
    "]).date\n",
    "\n",
    "# Mark rows where the date is either a holiday or special event\n",
    "df_anom['is_special'] = df_anom['date'].isin(holidays) | df_anom['date'].isin(special_event_dates)\n",
    "\n",
    "\n",
    "# Group by day type and calculate count, mean, and sum of anomaly counts\n",
    "summary = df_anom.groupby('is_special')['anomaly_count'] \\\n",
    "                 .agg(['count', 'mean', 'sum']) \\\n",
    "                 .rename(index={True: 'timestamp especial/festivo', False: 'timestamp normal'})\n",
    "\n",
    "print(\"Summary of anomalies on normal vs. special/holiday days:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea49278",
   "metadata": {},
   "source": [
    "#### Summary of Anomalies on Normal Days vs. Special/Holiday Days\n",
    "\n",
    "| Day Type              | Number of Records (`count`) | Average Anomalies per Record (`mean`) | Total Anomalies (`sum`) |\n",
    "|-----------------------|-----------------------------|---------------------------------------|------------------------|\n",
    "| Normal Day            | 6,279                       | 0.46                                  | 2,912                  |\n",
    "| Special/Holiday Day   | 576                         | 7.52                                  | 4,333                  |\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "- **Normal Day:**  \n",
    "  There are 6,279 records or timestamps on normal days.  \n",
    "  On average, each record has about 0.46 anomalies.  \n",
    "  In total, 2,912 anomalies were detected during these normal days.\n",
    "\n",
    "- **Special/Holiday Day:**  \n",
    "  There are 576 records on days that are holidays or special events.  \n",
    "  On average, each record shows many more anomalies, approximately 7.52.  \n",
    "  In total, 4,333 anomalies were detected during these relatively few special days.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Although the number of records on normal days is much larger, both the count and average number of anomalies are significantly higher on special or holiday days. This indicates that these days have a strong impact on the occurrence of anomalies, which is important for analysis or modeling that wants to consider the effect of special events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d76791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate anomalies per day (sum of anomaly_count grouped by date and is_special)\n",
    "daily_anomalies = df_anom.groupby(['date', 'is_special'])['anomaly_count'].sum().reset_index()\n",
    "print(daily_anomalies)\n",
    "# Now daily_anomalies has total anomalies per day, and whether the day is special\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(data=daily_anomalies, x='is_special', y='anomaly_count')\n",
    "plt.title(\"Distribution of Anomalies: Normal Days vs Special/Holiday Days\")\n",
    "plt.xlabel(\"Is Special/Holiday Day?\")\n",
    "plt.ylabel(\"Total Anomalous Sensors per Day\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4884a01",
   "metadata": {},
   "source": [
    "#### 5. Identification and Geographic Visualization of Most Frequent Anomalous Sensors During Peak Anomaly Timestamps\n",
    "\n",
    "In this part, we focus on identifying which sensors are most frequently anomalous during the top 10 timestamps with the highest total anomaly counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db258c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Select indices of top 10 timestamps with the highest number of anomalous sensors\n",
    "top_indices = torch.topk(anomalous_counts, k=10).indices\n",
    "\n",
    "# Initialize a counter for how many times each sensor was anomalous during those top timestamps\n",
    "sensor_counts = torch.zeros(anomalies.shape[1])\n",
    "\n",
    "# Sum the occurrences of anomalies per sensor at the top anomaly timestamps\n",
    "for idx in top_indices:\n",
    "    sensor_counts += anomalies[idx].float()\n",
    "\n",
    "# Identify the top 10 sensors that were most frequently anomalous during these peaks\n",
    "top_sensor_indices = torch.topk(sensor_counts, k=10).indices\n",
    "print(\"Sensors most frequently anomalous during anomaly peaks:\", top_sensor_indices.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import folium\n",
    "\n",
    "# #  Load sensor locations from CSV file\n",
    "# locations = pd.read_csv('data/sensor_graph/graph_sensor_locations.csv')\n",
    "\n",
    "# # Map sensor indices to sensor IDs\n",
    "# sensor_ids = locations['sensor_id'].tolist() \n",
    "# top_sensor_ids = [sensor_ids[i] for i in top_sensor_indices.tolist()]\n",
    "# anomalous_locations = locations[locations['sensor_id'].isin(top_sensor_ids)]\n",
    "\n",
    "# # Create a Folium map centered on Los Angeles\n",
    "# m = folium.Map(location=[34.05, -118.25], zoom_start=10)\n",
    "\n",
    "# # Add circle markers for each anomalous sensor location\n",
    "# for _, row in anomalous_locations.iterrows():\n",
    "#     folium.CircleMarker(\n",
    "#         location=[row['latitude'], row['longitude']],\n",
    "#         radius=5,\n",
    "#         popup=f\"Sensor {row['sensor_id']}\",\n",
    "#         color='red',\n",
    "#         fill=True,\n",
    "#         fill_opacity=0.7\n",
    "#     ).add_to(m)\n",
    "\n",
    "# # Save the map\n",
    "# m.save(\"sensores_anomalos.html\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da5500",
   "metadata": {},
   "source": [
    "Another option for identifying problematic sensors would be to count anomalies throughout the entire test data set, not just at anomaly peaks. This comprehensive approach allows you to detect sensors that exhibit recurring or intermittent faults outside of special events or peaks, which may reflect persistent technical problems or the need for preventive maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a5e94",
   "metadata": {},
   "source": [
    "I would not add the maps to the project at this time, but I am leaving the code in case we want to do so at some point. EVERYTHING THAT FOLLOWS IS A DRAFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# mse_matrix: [n_muestras, n_sensores] (ej: [6720, 207])\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(mse_matrix.T.numpy(), cmap=\"viridis\", cbar=True)\n",
    "plt.xlabel(\"Timestamps (cada 5 min)\")\n",
    "plt.ylabel(\"Sensores\")\n",
    "plt.title(\"Mapa de calor del MSE por sensor\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0abab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# anomalies: [n_muestras, 207]\n",
    "anomaly_matrix = anomalies.numpy().T  # ahora [207, n_muestras]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(anomaly_matrix, cmap=\"Reds\", cbar=True)\n",
    "plt.title(\"Mapa de calor de anomalÃ­as por sensor y tiempo\")\n",
    "plt.xlabel(\"Muestra (cada 5 min)\")\n",
    "plt.ylabel(\"Sensor\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_sample = torch.tensor(x_test[100:101])\n",
    "    c_normal = torch.tensor(c_test[100:101])\n",
    "    c_fake = torch.zeros_like(c_normal)  # condiciones \"neutrales\"\n",
    "\n",
    "    recon_normal = model(x_sample, c_normal)[0]\n",
    "    recon_fake = model(x_sample, c_fake)[0]\n",
    "\n",
    "    plt.plot(x_sample.numpy().flatten(), label=\"Real\")\n",
    "    plt.plot(recon_normal.numpy().flatten(), label=\"Con condiciones\")\n",
    "    plt.plot(recon_fake.numpy().flatten(), label=\"Sin condiciones\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Impacto de las variables condicionales\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32192e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(reconstruction_errors, bins=50)\n",
    "# plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold = {threshold:.4f}')\n",
    "# plt.title(\"DistribuciÃ³n del error de reconstrucciÃ³n\")\n",
    "# plt.xlabel(\"Error\")\n",
    "# plt.ylabel(\"Frecuencia\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896395f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(reconstruction_errors, label='Error de reconstrucciÃ³n')\n",
    "# plt.axhline(threshold, color='red', linestyle='--', label='Umbral (95%)')\n",
    "# plt.scatter(np.where(anomalies)[0], np.array(reconstruction_errors)[anomalies], color='orange', label='AnomalÃ­as')\n",
    "# plt.legend()\n",
    "# plt.title(\"Errores de reconstrucciÃ³n y anomalÃ­as detectadas\")\n",
    "# plt.xlabel(\"Ãndice de muestra\")\n",
    "# plt.ylabel(\"MSE\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e8288",
   "metadata": {},
   "source": [
    "#### Sliding Windows\n",
    "\n",
    "Sliding windows allow us to structure the time series data as sequences of fixed length, suitable for feeding into our VAE model.\n",
    "\n",
    "The window size of 12 means each sample contains 12 consecutive timesteps (i.e., one hour of data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sliding_windows(data, window_size):\n",
    "#     X = []\n",
    "#     # for i in range(len(data) - window_size):\n",
    "#     #     X.append(data[i:i+window_size])\n",
    "#     i: int = 0\n",
    "#     while i < 34272:\n",
    "#         X.append(data[i:i+window_size])\n",
    "#         i= i + 12\n",
    "#     return np.array(X)\n",
    "\n",
    "# window_size = 12  # 12 pasos = 1 hora\n",
    "# X = create_sliding_windows(data_normalized, window_size)  # shape: (34260, 12, 207)\n",
    "# # 12 = rows\n",
    "# # 34260 =  \n",
    "# # \n",
    "# print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ba1e8",
   "metadata": {},
   "source": [
    "## CVAE + Long Short Term Memory (LSTM) reconstruction in latent space\n",
    "\n",
    "A **LSTM** (Long Short-Term Memory) is a type of recurrent neural network designed to model sequences and capture long-term dependencies through gated mechanisms that control information flow over time.\n",
    "\n",
    "The **CVAE + LSTM** model extends the CVAE by introducing an LSTM that operates in the latent space. The CVAE encodes each timestep of a traffic speed sequence into an embedding; the LSTM then models the temporal evolution of these embeddings, predicting the last window_size-1 embeddings using the first window_size -1 ones. Only the **last predicted embedding** is decoded, summarizing the full sequence for reconstruction.\n",
    "\n",
    "**Trainings** of CVAE and LSTM are kept separate: this improves stability and modularity:\n",
    "* **Decoupled objectives**: The CVAE learns to encode/reconstruct, while the LSTM models temporal dynamics. Joint training can lead to conflicting gradients and unstable convergence.\n",
    "* **Better control**: The encoder can be validated independently for embedding quality before feeding sequences to the LSTM.\n",
    "* **Modularity**: Once trained, components can be reused or replaced independently (e.g., testing different sequence models on the same embeddings).\n",
    "* **Simpler debugging**: Isolating errors or performance issues is easier when training in stages.\n",
    "\n",
    "End-to-end training could be explored later for fine-tuning, once both parts perform well individually {????????????????}\n",
    "\n",
    "> **Why use LSTM inside a CVAE for anomaly detection on traffic speed data?**\n",
    "> VAE works out for indipendent input data, while time series are characterized by innate sequential dependencies. The inception of a LSTM modul inside the original CVAE allows to account for these dependencies.\n",
    "\n",
    "### Main Components\n",
    "\n",
    "* **Encoder (`x`)**: Maps each timestep of input to a latent embedding.\n",
    "* **Condition (`c`)**: Provides context (e.g., time, day type) to guide encoding and decoding.\n",
    "* **LSTM**: Models the sequence of embeddings and predicts the next (last) embedding.\n",
    "* **Decoder**: Reconstructs the final output from the last predicted embedding.\n",
    "\n",
    "### In the next sections\n",
    "\n",
    "First, we present the LSTM model definition, the preprocessing of CVAE-embeddings as LSTM input data,  and the LSTM training over the train data set. The we move to the CVAE_LSTM model definition and we assess its performance over the test dataset {by?????}. Reconstructed time series are plotted. \n",
    "Finally, the score function matrix is build, where scores for each sensor are provided over a customizable window_size. According to a percentile threshold, sensors are flagged as anomalous or not over precise lenght windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b883d4",
   "metadata": {},
   "source": [
    "### LSTM model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2efb0b",
   "metadata": {},
   "source": [
    "#### 1. LSTM model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LSTM ---\n",
    "class LSTM_Predictor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based predictor for sequences of latent embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding_dim (int): Dimensionality of each embedding in the sequence.\n",
    "        hidden_dim (int): Number of hidden units in the LSTM.\n",
    "        num_layers (int): Number of stacked LSTM layers.\n",
    "        dropout_prob (float): Dropout probability (applied if num_layers > 1). For regularization.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Returns predicted embedding sequence.\n",
    "        compute_loss(predicted_sequence, target_last_embedding): Computes MSE loss on last predicted embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers=1, dropout_prob=0.1):\n",
    "        super(LSTM_Predictor, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM processes input sequences of embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, # input output LSTM: (batch, seq, feature).\n",
    "            dropout=dropout_prob if num_layers > 1 else 0\n",
    "        )\n",
    "        # Linear layer maps LSTM outputs back to embedding space\n",
    "        self.fc = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "        # MSE loss for comparing predicted and true embeddings\n",
    "        self.criterion = nn.MSELoss() #draft3 +\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM and linear layer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input of shape (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        Returns:\n",
    "            predictions (Tensor): Predicted embeddings, shape (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        lstm_out, _ = self.lstm(x) # LSTM output: (batch, seq_len, hidden_dim)\n",
    "        predictions = self.fc(lstm_out) # Map to embedding_dim\n",
    "        return predictions\n",
    "    \n",
    "    # loss : draft3 +\n",
    "    def compute_loss(self, predicted_sequence, target_last_embedding):\n",
    "        \"\"\"\n",
    "        Compute MSE loss between the last predicted embedding and the true last embedding.\n",
    "\n",
    "        Args:\n",
    "            predicted_sequence (Tensor): Output from forward(), shape (batch_size, seq_len, embedding_dim)\n",
    "            target_last_embedding (Tensor): True last embedding, shape (batch_size, embedding_dim)\n",
    "\n",
    "        Returns:\n",
    "            loss (Tensor): Scalar MSE loss\n",
    "        \"\"\"\n",
    "        predicted_last = predicted_sequence[:, -1, :] #Last time step prediction\n",
    "        loss = self.criterion(predicted_last, target_last_embedding)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b5298",
   "metadata": {},
   "source": [
    "#### 2. From raw to LSTM input data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf62785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all latent embeddings (mu) from the trained VAE for both train and test sets\n",
    "def get_embeddings(model, data_loader, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, c_batch in data_loader:\n",
    "            x_batch, c_batch = x_batch.to(device), c_batch.to(device) \n",
    "            mu, _ = model.encode(x_batch, c_batch)\n",
    "            embeddings.append(mu.cpu())\n",
    "    return torch.cat(embeddings, dim=0) \n",
    "\n",
    "# Redefine train_loader with shuffle=False to preserve temporal order for sequential embeddings\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=False) \n",
    "print(len(train_loader))\n",
    "\n",
    "# Extract embeddings for train and test sets\n",
    "embeddings_train = get_embeddings(model, train_loader, device)\n",
    "embeddings_test = get_embeddings(model, test_loader, device)\n",
    "print(\"Embeddings train:\", embeddings_train.shape)\n",
    "print(\"Embeddings test:\", embeddings_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3dfdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sliding windows from embeddings for LSTM input/target\n",
    "def create_sliding_windows(samples, window_size=12):\n",
    "    \"\"\"\n",
    "    Given a tensor of shape [N, D], returns a tensor of shape [num_windows, window_size, D],\n",
    "    where each window contains 'window_size' consecutive embeddings.\n",
    "    Windows are created with stride 1 (fully overlapping).\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    # Generate all possible windows of length 'window_size'\n",
    "    for i in range(len(samples) - window_size):\n",
    "        windows.append(samples[i:i + window_size])\n",
    "    return torch.stack(windows)\n",
    "\n",
    "# window_size is default 12 steps = 1 hour of data per window\n",
    "train_windows = create_sliding_windows(embeddings_train)\n",
    "test_windows = create_sliding_windows(embeddings_test)\n",
    "\n",
    "print(train_windows.shape)  # [num_train_windows, 12, latent_dim]\n",
    "print(test_windows.shape)   # [num_test_windows, 12, latent_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5155ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Prepare input and target tensors for LSTM training\n",
    "# Input: first 11 embeddings in each window (e_1, ..., e_11)\n",
    "X_train_lstm = train_windows[:, :11, :]\n",
    "# Target: the last embedding in each window (e_12), used for loss computation\n",
    "Y_train_lstm_for_loss = train_windows[:, -1, :]\n",
    "\n",
    "X_test_lstm = test_windows[:, :11, :]\n",
    "Y_test_lstm_for_loss = test_windows[:, -1, :]\n",
    "\n",
    "print(f\"Shape of X_train_lstm: {X_train_lstm.shape}\")\n",
    "print(f\"Shape of Y_train_lstm_for_loss: {Y_train_lstm_for_loss.shape}\")\n",
    "\n",
    "# Create DataLoaders for LSTM training and evaluation\n",
    "batch_size_lstm = 64  # You can adjust this batch size\n",
    "train_dataset_lstm = TensorDataset(X_train_lstm, Y_train_lstm_for_loss)\n",
    "train_loader_lstm = DataLoader(train_dataset_lstm, batch_size=batch_size_lstm, shuffle=False) # COULD WE USE SHUFFLE? WOULD IT MEAN SHUFFLING TEMPORAL WINDOWS (KEEPING THE SEQUENCE ORDER WITHIN)?\n",
    "test_dataset_lstm = TensorDataset(X_test_lstm, Y_test_lstm_for_loss)\n",
    "test_loader_lstm = DataLoader(test_dataset_lstm, batch_size=batch_size_lstm, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f615f93",
   "metadata": {},
   "source": [
    "#### 3. LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM training\n",
    "embedding_dim = X_train_lstm.shape[2]\n",
    "\n",
    "# Tune the following as needed \n",
    "hidden_dim_lstm = 64    # Esempio, da ottimizzare\n",
    "num_layers_lstm = 2     \n",
    "learning_rate_lstm = 0.001 \n",
    "num_epochs_lstm = 50    \n",
    "#dropout_prob_lstm  = 0.2 \n",
    "\n",
    "# Initialize LSTM model and optimizer\n",
    "lstm_model = LSTM_Predictor(embedding_dim, hidden_dim_lstm, num_layers_lstm, dropout_prob=0.2).to(device)\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=learning_rate_lstm)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(num_epochs_lstm):\n",
    "    lstm_model.train()  \n",
    "    epoch_loss = 0.0\n",
    "    for batch_X, batch_Y_target_last in train_loader_lstm:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y_target_last = batch_Y_target_last.to(device)\n",
    "        optimizer_lstm.zero_grad()\n",
    "        predicted_sequence = lstm_model(batch_X) # Forward pass: predict the sequence of embeddings\n",
    "        loss = lstm_model.compute_loss(predicted_sequence, batch_Y_target_last)\n",
    "        loss.backward()\n",
    "        optimizer_lstm.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss / len(train_loader_lstm)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_lstm}], LSTM Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3ab1e",
   "metadata": {},
   "source": [
    "#### 4. LSTM output embeddings over train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3cb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Evaluate LSTM on test embeddings and collect predictions\n",
    "lstm_out_embeddings = []\n",
    "\n",
    "lstm_model.eval()  \n",
    "with torch.no_grad():\n",
    "    test_losses = []\n",
    "    for batch_X, batch_Y_target_last in test_loader_lstm:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_Y_target_last = batch_Y_target_last.to(device)\n",
    "        predicted_sequence = lstm_model(batch_X)\n",
    "        loss = lstm_model.compute_loss(predicted_sequence, batch_Y_target_last)\n",
    "        test_losses.append(loss.item())\n",
    "        lstm_out_embeddings.append(predicted_sequence[:, -1, :].cpu())\n",
    "    \n",
    "# Compute average loss over the test set\n",
    "avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "print(f\"Test Loss LSTM: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Concatenate all predictions into a single tensor\n",
    "lstm_out_embeddings = torch.cat(lstm_out_embeddings, dim=0) \n",
    "print(\"Shape of LSTM predictions:\", lstm_out_embeddings.shape)  # [num_test_windows, embedding_dim]\n",
    "print(embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec0515",
   "metadata": {},
   "source": [
    "### CVAE_LSTM model, outputs and visualisation\n",
    "#### 1. CVAE_LSTM model and training over train dataset\n",
    "\n",
    "During inference, we feed only the mean vector $\\mu$ to the decoder (by setting `is_code_input=True`). Doing this bypasses the stochastic sampling $z = \\mu + \\sigma \\odot \\epsilon$, making reconstruction fully deterministic. The Gaussian sampling during training regularizes the latent spaceâ€”encouraging $\\mu \\sim \\mathcal{N}(0,I)$ and preventing overfittingâ€”but is not required at inference time. When the LSTM predicts a latent $\\mu$, we want a stable, reproducible reconstruction; adding noise on each $\\mu$ would yield blurred or random outputs. Using a VAE instead of a standard autoencoder ensures that even unseen $\\mu$ values lie in a well-structured latent space, so the decoder can reliably reconstruct them without stochasticity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CVAE + LSTM ---\n",
    "class CVAE_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    This model extends the standard CVAE by integrating a temporal LSTM module in the latent space.\n",
    "    During evaluation, the LSTM predicts the next latent embedding from a sequence of previous embeddings, \n",
    "    allowing the decoder to reconstruct x based on temporally-aware latent dynamics.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, cond_dim, latent_dim=16):\n",
    "        super(CVAE_LSTM, self).__init__()\n",
    "\n",
    "        # Encoder: concatenates x and c, then passes through two hidden layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, 128), # First dense layer\n",
    "            nn.ReLU(),                            # Non-linearity\n",
    "            nn.Linear(128, 64),                   # Second dense layer\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Latent space: outputs mean (mu) and log-variance (logvar) for z\n",
    "        self.fc_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        # Decoder: reconstructs x from latent z and condition c\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim) # Output matches original x dimension\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat([x, c], dim=1))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        return self.decoder(torch.cat([z, c], dim=1))\n",
    "\n",
    "    def forward(self, x, c, lstm_model=None): \n",
    "        \"\"\"\n",
    "        Forward pass for CVAE_LSTM:\n",
    "        1) Encode (x, c) to get mu and logvar.\n",
    "        2) Optionally, in eval mode, use LSTM to predict mu from a sequence of previous mus.\n",
    "        3) Sample z using the reparameterization trick.\n",
    "        4) Decode (z, c) to reconstruct x.\n",
    "\n",
    "        Args:\n",
    "            x: Input data\n",
    "            c: Conditional features\n",
    "            lstm_model: Trained LSTM model for temporal prediction (optional, used in eval)\n",
    "            mu_queue: Sequence buffer for previous mus (optional, used in eval)\n",
    "            window_size: Length of the mu sequence for LSTM (default: 12)\n",
    "\n",
    "        Returns:\n",
    "            x_recon: Reconstructed input\n",
    "            mu: Mean of latent distribution\n",
    "            logvar: Log-variance of latent distribution\n",
    "        \"\"\"\n",
    "        # # preliminari debugging:\n",
    "        # print(x.shape, c.shape)\n",
    "        # 1) Encode input and condition to latent parameters\n",
    "        mu, logvar = self.encode(x, c)\n",
    "\n",
    "        # 2) In evaluation mode, optionally use LSTM to predict mu from previous mus\n",
    "        if (not self.training) and (lstm_model is not None):   #only if CVAE is in eval mode and lstm_model and mu_queue are provided\n",
    "            # debugging:\n",
    "            print(\"CVAE in eval mode, applying LSTM to mu sequence\")\n",
    "            lstm_model.eval()  # LSTM in eval mode\n",
    "            with torch.no_grad():\n",
    "                lstm_out = lstm_model(mu)\n",
    "                #debug\n",
    "                print(\"LSTM output shape:\", lstm_out.shape)\n",
    "            x_recon_seq = self.decode(lstm_out, c)  # Decode using the predicted mu\n",
    "            print(\"Shape of reconstructed sequence from LSTM:\", x_recon_seq.shape)\n",
    "            x_recon = x_recon_seq[-1, :] # Keep last output from the predicted sequence...NON SICURA DELLE SIZE \n",
    "            print (\"Reconstruction shape from LSTM output:\", x_recon.shape)      \n",
    "            return x_recon, mu, logvar   # possible to return also mu, logvar \n",
    "        \n",
    "        else: \n",
    "            # 2) Sample z using reparameterization\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            \n",
    "            # 3) Decode the sampled z (with c) â†’ reconstruction\n",
    "            x_recon = self.decode(z, c)\n",
    "            return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# training of CVAE_LSTM model over non-overlapping batches of original train data (VOGLIAMO CAMBIARE?)\n",
    "\n",
    "# Input dimensions from the training data:\n",
    "input_dim = x_train.shape[1]\n",
    "cond_dim = c_train.shape[1]\n",
    "\n",
    "latent_dim = 16\n",
    " \n",
    "model_cvae_lstm = CVAE_LSTM(input_dim, cond_dim, latent_dim).to(device)\n",
    "\n",
    "# Create an Adam optimizer to update all model parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_recon, total_kl = 0, 0, 0\n",
    "\n",
    "    # Iterate over the training data in mini-batches provided by train_loader\n",
    "    for x_batch, c_batch in train_loader:\n",
    "        x_batch, c_batch = x_batch.to(device), c_batch.to(device)\n",
    "\n",
    "        # Zero out all accumulated gradients before computing new ones\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the reconstruction, plus mu and logvar from the encoder\n",
    "        x_recon, mu, logvar = model_cvae_lstm(x_batch, c_batch)\n",
    "\n",
    "        # CVAE loss:\n",
    "        loss, recon_loss, kl_loss = loss_function(x_recon, x_batch, mu, logvar)\n",
    "        \n",
    "        # Backward pass: compute gradients of loss w.r.t. model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters using the computed gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the summed losses across all examples in this batch\n",
    "        total_loss += loss.item() * x_batch.size(0)\n",
    "        total_recon += recon_loss.item() * x_batch.size(0)\n",
    "        total_kl += kl_loss.item() * x_batch.size(0)\n",
    "    \n",
    "    # After finishing all batches in this epoch, compute the average loss per example\n",
    "    dataset_size = len(train_loader.dataset)\n",
    "    avg_loss  = total_loss  / dataset_size\n",
    "    avg_recon = total_recon / dataset_size\n",
    "    avg_kl    = total_kl    / dataset_size\n",
    "\n",
    "    # Print out the epoch summary: average total loss, reconstruction loss, and KL loss\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "          f\"Loss: {avg_loss:.4f} | \"\n",
    "          f\"Recon: {avg_recon:.4f} | \"\n",
    "          f\"KL: {avg_kl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b2a53",
   "metadata": {},
   "source": [
    "#### 2. CVAE_LSTM evaluation on test set, and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f33bc3",
   "metadata": {},
   "source": [
    "Over the original dataset, we rebuild batches as sliding windows of lenght 'window_size'. This is to manage compatibility among CVAE and LSTM inputs/outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed73559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, c, window):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.c = torch.tensor(c, dtype=torch.float32)\n",
    "        self.window = window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x) - self.window\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index:index+self.window], self.c[index:index+self.window]\n",
    "\n",
    "\n",
    "window_size_cvae_lstm = 11  #LSTM input and output sequence length (11 previous embeddings to predict the next one)\n",
    "\n",
    "input_cvae_lstm = MyDataset(x_test, c_test, window_size_cvae_lstm)\n",
    "input_cvae_lstm_loader = DataLoader(input_cvae_lstm, batch_size=1, shuffle=False)  # No shuffling to preserve temporal order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6dad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CVAE_LSTM model on the test set using LSTM-driven latent dynamics. Batches are overlapping sequences of lenght window_size.\n",
    "\n",
    "\"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "vae_lstm_recon = []\n",
    "vae_lstm_originals = []\n",
    "\n",
    "\n",
    "model_cvae_lstm.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for x_batch, c_batch in input_cvae_lstm_loader:\n",
    "        x_seq = x_batch.squeeze(0)  # from [batch,window, 207] to [window, 207]\n",
    "        c_seq = c_batch.squeeze(0)  # from [batch,window, 6] to [window,6]\n",
    "        x_seq, c_seq = x_seq.to(device), c_seq.to(device)\n",
    "        # Forward pass: use LSTM to predict temporally-aware latent embedding\n",
    "        x_recon, mu, logvar = model_cvae_lstm(\n",
    "            x_seq, c_seq, \n",
    "            lstm_model=lstm_model\n",
    "            )\n",
    "        vae_lstm_recon.append(x_recon.cpu())\n",
    "        vae_lstm_originals.append(x_batch.cpu())\n",
    "\n",
    "vae_lstm_recon_all = torch.cat(vae_lstm_recon, dim=0)     # [Nw_test, 207]\n",
    "print(\"vae_lstm_recon_all shape\", vae_lstm_recon_all.shape)\n",
    "vae_lstm_originals = torch.cat(vae_lstm_originals, dim=0) # [Nw_test, 207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of reconstructed time series for 3 sensors against the original ones of the same sensors (test set)\n",
    "specific_sensors = [\"716339\", \"765164\", \"716328\"]\n",
    "vae_lstm_recon_all_ = pd.DataFrame(vae_lstm_recon_all.numpy(), columns=df.columns) \n",
    "recon_time_series = plot_random_sensors(vae_lstm_recon_all_, num_sensors=5, sens=specific_sensors)\n",
    "\n",
    "vae_lstm_originals_ = pd.DataFrame(vae_lstm_originals.numpy(), columns=df.columns) \n",
    "orig_time_series = plot_random_sensors(vae_lstm_originals_, num_sensors=5, sens=specific_sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79fdd1",
   "metadata": {},
   "source": [
    "### SCORE FUNCTION and Results Visualization\n",
    "...\n",
    "\n",
    "We use the score function to compute the MSE between reconstruction and real data for each sensor and over defined time windows. This allows us to identify anomalous sensors when the error exceeds a percentile threshold, even over prolonged periods. In the future, it would be interesting to explore a composite score that considers both the magnitude of the anomaly and the number of anomalous sensors per window, for a more robust detection.\n",
    "\n",
    ">$$\n",
    "\\text{Score}_{t \\in W,s} = \\sum_{t} \\left(x^{\\text{recon}}_{t,s} - x^{\\text{orig}}_{t,s} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE as a score function to spot anomalies, mse per sensor and over window of lenght windth_size\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def score_function(x_recon, x_orig, window_size=12, percentile=99.5, index=None):\n",
    "    \"\"\"\n",
    "    Compute the per-sensor MSE between reconstructed and original data, and a rolling mean over a window.\n",
    "    Returns:\n",
    "        mse_matrix: DataFrame of raw MSE values (samples x sensors)\n",
    "        mse_windows: DataFrame of rolling mean MSE (windowed)\n",
    "        thresholds: Array of per-sensor thresholds (percentile over all MSEs)\n",
    "        thresholds_windows: Array of per-sensor thresholds (percentile over windowed MSEs)\n",
    "    \"\"\"\n",
    "    # Ensure x_recon and x_orig have the same number of samples\n",
    "    dif = x_recon.shape[0] - x_orig.shape[0]\n",
    "    if dif > 0:\n",
    "        x_recon = x_recon[:x_orig.shape[0]] \n",
    "    elif dif < 0:\n",
    "        x_orig = x_orig[-x_recon.shape[0]:]\n",
    "        \n",
    "    # Compute per-sample, per-sensor MSE\n",
    "    mse_matrix = (x_recon - x_orig) ** 2  \n",
    "    mse_matrix = pd.DataFrame(mse_matrix, index=x_orig.index, columns=x_orig.columns)\n",
    "\n",
    "    # Compute rolling mean MSE over the specified window size \n",
    "    mse_windows = mse_matrix.rolling(window=window_size, min_periods=window_size).mean() \n",
    "\n",
    "    # Compute anomaly thresholds as the given percentile for each sensor (over a window)\n",
    "    thresholds = np.percentile(mse_matrix.values, percentile, axis=0) \n",
    "    thresholds_windows = np.percentile(mse_windows.values, percentile, axis=0) \n",
    "\n",
    "    return mse_matrix , mse_windows , thresholds, thresholds_windows\n",
    "\n",
    "\n",
    "# Prepare indexes and columns for DataFrames\n",
    "df_time_train, df_time_test = train_test_split(df_time, test_size=0.2, shuffle=False)\n",
    "index = df_time_test.index  # Index for test set\n",
    "index_lstm_decoded = df_time_test.index  # Index for windowed output .....before was index_lstm_decoded = df_time_test.index[window_size-1:] CHECK\n",
    "columns = df.columns\n",
    "\n",
    "# Convert decoded LSTM outputs and test data to DataFrames with correct indices\n",
    "vae_lstm_recon_all_pd = pd.DataFrame(vae_lstm_recon_all, columns=columns, index=index_lstm_decoded) \n",
    "vae_lstm_originals_pd = pd.DataFrame(vae_lstm_originals, columns=columns, index=index)  \n",
    "\n",
    "# Compute MSE matrices and thresholds\n",
    "mse_matrix, mse_windows, thresh, thresh_windows = score_function(vae_lstm_recon_all_pd, vae_lstm_originals_pd, window_size=1, percentile=99.5)\n",
    "print(mse_matrix.head())      # Show first rows of raw MSE matrix\n",
    "print(mse_windows.head())     # Show first rows of rolling mean MSE matrix\n",
    "print(thresh)                 # Print per-sensor thresholds (raw)\n",
    "print(thresh_windows)         # Print per-sensor thresholds (windowed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sci-env)",
   "language": "python",
   "name": "sci-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
