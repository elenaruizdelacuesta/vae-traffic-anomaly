@report{,
   abstract = {As core thermal power generation equipment, steam turbines incur significant expenses and adverse effects on operation when facing interruptions like downtime, maintenance, and damage. Accurate anomaly detection is the prerequisite for ensuring the safe and stable operation of steam turbines. However, challenges in steam turbine anomaly detection, including inherent anomalies, lack of temporal information analysis, and high-dimensional data complexity, limit the effectiveness of existing methods. To address these challenges, we proposed an Enhanced Long Short-Term Memory Variational Autoencoder using Deep Advanced Features and Gaussian Mixture Model (ELSTMVAE-DAF-GMM) for precise unsupervised anomaly detection in unlabeled datasets. Specifically, LSTMVAE, integrating LSTM with VAE, was used to project high-dimensional time-series data to a low-dimensional phase space. The Deep Autoencoder-Local Outlier Factor (DAE-LOF) sample selection mechanism was used to eliminate inherent anomalies during training, further improving the model's precision and reliability. The novel deep advanced features (DAF) hybridize latent embeddings and reconstruction discrepancies from the LSTMVAE model and provide a more comprehensive data representation within a continuous and structured phase space, significantly enhancing anomaly detection by synergizing temporal dynamics with data pattern variations. These DAF were incorporated into GMM to ensure robust and effective unsupervised anomaly detection. We utilized real operating data from industry steam turbines and conducted both comparison and ablation experiments, demonstrating superior anomaly detection outcomes characterized by high accuracy and minimal false alarm rates compared with existing methods.},
   author = {Weiming Xu and Peng Zhang},
   keywords = {Gaussian mixture model,Long short-term memory,Steam turbine,Unsupervised anomaly detection,Variational autoencoder},
   note = {Keywords:  Unsupervised  anomaly  detection;  Long  short-term  memory; Variational autoencoder; Gaussian mixture model; Steam turbine},
   title = {Steam Turbine Anomaly Detection: An Unsupervised Learning Approach Using Enhanced Long Short-Term Memory Variational Autoencoder},
}
@article{Nguyen2019,
   abstract = {This paper looks into the problem of detecting network anomalies by analyzing NetFlow records. While many previous works have used statistical models and machine learning techniques in a supervised way, such solutions have the limitations that they require large amount of labeled data for training and are unlikely to detect zero-day attacks. Existing anomaly detection solutions also do not provide an easy way to explain or identify attacks in the anomalous traffic. To address these limitations, we develop and present GEE, a framework for detecting and explaining anomalies in network traffic. GEE comprises of two components: (i) Variational Autoencoder (VAE) - an unsupervised deep-learning technique for detecting anomalies, and (ii) a gradient-based fingerprinting technique for explaining anomalies. Evaluation of GEE on the recent UGR dataset demonstrates that our approach is effective in detecting different anomalies as well as identifying fingerprints that are good representations of these various attacks.},
   author = {Quoc Phong Nguyen and Kar Wai Lim and Dinil Mon Divakaran and Kian Hsiang Low and Mun Choon Chan},
   month = {3},
   note = {Index Terms—Anomaly Detection, NetFlow Records, Gradient-based  Fingerprinting},
   title = {GEE: A Gradient-based Explainable Variational Autoencoder for Network Anomaly Detection},
   url = {http://arxiv.org/abs/1903.06661},
   year = {2019},
}
@article{Boquet2020,
   abstract = {Efforts devoted to mitigate the effects of road traffic congestion have been conducted since 1970s. Nowadays, there is a need for prominent solutions capable of mining information from messy and multidimensional road traffic data sets with few modeling constraints. In that sense, we propose a unique and versatile model to address different major challenges of traffic forecasting in an unsupervised manner. We formulate the road traffic forecasting problem as a latent variable model, assuming that traffic data is not generated randomly but from a latent space with fewer dimensions containing the underlying characteristics of traffic. We solve the problem by proposing a variational autoencoder (VAE) model to learn how traffic data are generated and inferred, while validating it against three different real-world traffic data sets. Under this framework, we propose an online unsupervised imputation method for unobserved traffic data with missing values. Additionally, taking advantage of the low dimension latent space learned, we compress the traffic data before applying a prediction model obtaining improvements in the forecasting accuracy. Finally, given that the model not only learns useful forecasting features but also meaningful characteristics, we explore the latent space as a tool for model and data selection and traffic anomaly detection from the point of view of traffic modelers.},
   author = {Guillem Boquet and Antoni Morell and Javier Serrano and Jose Lopez Vicario},
   doi = {10.1016/j.trc.2020.102622},
   issn = {0968090X},
   journal = {Transportation Research Part C: Emerging Technologies},
   keywords = {Anomaly detection,Dimension reduction,Intelligent transportation systems,Missing data imputation,Model selection,Traffic forecasting},
   month = {6},
   note = {"day samples were projected to a 100-dimension latent space learned by the VAE modelin an unsupervised manner. Then, the two principalcomponents(PC) of the projected data were plotted with the help of PCA."<br/><br/>automatic anomaly detection inspected visually},
   publisher = {Elsevier Ltd},
   title = {A variational autoencoder solution for road traffic forecasting systems: Missing data imputation, dimension reduction, model selection and anomaly detection},
   volume = {115},
   year = {2020},
}
@book{,
   abstract = {The ICASSP meeting is the world s largest and most comprehensive technical conference focused on signal processing and its applications The conference will feature world class speakers, tutorials, exhibits, and over 50 lecture and poster sessions. },
   author = {Ronald Clark2, Robert Birke3, Sandro Sch ̈onborn3, Niki Trigoni1, Stephen Roberts Shuyu Lin1},
   isbn = {9781509066315},
   note = {Index Terms—Anomaly Detection,  Time Series,  DeepLearning, Unsupervised Learning},
   pages = {9304},
   publisher = {IEEE},
   title = {ANOMALY DETECTION FOR TIME SERIES USING VAE-LSTM HYBRID MODEL},
   year = {2020},
}
@inproceedings{Shao2022,
   abstract = {Multivariate Time Series (MTS) forecasting plays a vital role in a wide range of applications. Recently, Spatial-Temporal Graph Neural Networks (STGNNs) have become increasingly popular MTS forecasting methods. STGNNs jointly model the spatial and temporal patterns of MTS through graph neural networks and sequential models, significantly improving the prediction accuracy. But limited by model complexity, most STGNNs only consider short-term historical MTS data, such as data over the past one hour. However, the patterns of time series and the dependencies between them (i.e., the temporal and spatial patterns) need to be analyzed based on long-term historical MTS data. To address this issue, we propose a novel framework, in which STGNN is Enhanced by a scalable time series Pre-training model (STEP). Specifically, we design a pre-training model to efficiently learn temporal patterns from very long-term history time series (e.g., the past two weeks) and generate segment-level representations. These representations provide contextual information for short-term time series input to STGNNs and facilitate modeling dependencies between time series. Experiments on three public real-world datasets demonstrate that our framework is capable of significantly enhancing downstream STGNNs, and our pre-training model aptly captures temporal patterns.},
   author = {Zezhi Shao and Zhao Zhang and Fei Wang and Yongjun Xu},
   doi = {10.1145/3534678.3539396},
   isbn = {9781450393850},
   journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   keywords = {multivariate time series forecasting,pre-training model,spatial-temporal graph neural network},
   month = {8},
   note = {KEYWORDSmultivariate time series forecasting, spatial-temporal graph neuralnetwork, pre-training model<br/><br/>METR-LA dataset },
   pages = {1567-1577},
   publisher = {Association for Computing Machinery},
   title = {Pre-training Enhanced Spatial-temporal Graph Neural Network for Multivariate Time Series Forecasting},
   year = {2022},
}
@article{Li2022,
   abstract = {Recent studies have shown that autoencoder-based models can achieve superior performance on anomaly detection tasks due to their excellent ability to fit complex data in an unsupervised manner. In this work, we propose a novel autoencoder-based model, named StackVAE-G that can significantly bring the efficiency and interpretability to multivariate time series anomaly detection. Specifically, we utilize the similarities across the time series channels by the stacking block-wise reconstruction with a weight-sharing scheme to reduce the size of learned models and also relieve the overfitting to unknown noises in the training data. We also leverage a graph learning module to learn a sparse adjacency matrix to explicitly capture the stable interrelation structure among multiple time series channels for the interpretable pattern reconstruction of interrelated channels. Combining these two modules, we introduce the stacking block-wise VAE (variational autoencoder) with GNN (graph neural network) model for multivariate time series anomaly detection. We conduct extensive experiments on three commonly used public datasets, showing that our model achieves comparable (even better) performance with the state-of-the-art models and meanwhile requires much less computation and memory cost. Furthermore, we demonstrate that the adjacency matrix learned by our model accurately captures the interrelation among multiple channels, and can provide valuable information for failure diagnosis applications.},
   author = {Wenkai Li and Wenbo Hu and Ting Chen and Ning Chen and Cheng Feng},
   doi = {10.1016/j.aiopen.2022.07.001},
   issn = {26666510},
   journal = {AI Open},
   keywords = {Anomaly detection,Autoencoders,Graph neural network,Time-series},
   month = {1},
   note = {Keywords:Time-seriesAnomaly detectionAutoencodersGraph neural network},
   pages = {101-110},
   publisher = {KeAi Communications Co.},
   title = {StackVAE-G: An efficient and interpretable model for time series anomaly detection},
   volume = {3},
   year = {2022},
}
@article{Templin2023,
   abstract = {Deep generative learning cannot only be used for generating new data with statistical characteristics derived from input data but also for anomaly detection, by separating nominal and anomalous instances based on their reconstruction quality. In this paper, we explore the performance of three unsupervised deep generative models -- variational autoencoders (VAEs) with Gaussian, Bernoulli, and Boltzmann priors -- in detecting anomalies in flight-operations data of commercial flights consisting of multivariate time series. We devised two VAE models with discrete latent variables (DVAEs), one with a factorized Bernoulli prior and one with a restricted Boltzmann machine (RBM) as prior, because of the demand for discrete-variable models in machine-learning applications and because the integration of quantum devices based on two-level quantum systems requires such models. The DVAE with RBM prior, using a relatively simple -- and classically or quantum-mechanically enhanceable -- sampling technique for the evolution of the RBM's negative phase, performed better than the Bernoulli DVAE and on par with the Gaussian model, which has a continuous latent space. Our studies demonstrate the competitiveness of a discrete deep generative model with its Gaussian counterpart on anomaly-detection tasks. Moreover, the DVAE model with RBM prior can be easily integrated with quantum sampling by outsourcing its generative process to measurements of quantum states obtained from a quantum annealer or gate-model device.},
   author = {Thomas Templin and Milad Memarzadeh and Walter Vinci and P. Aaron Lott and Ata Akbari Asanjan and Anthony Alexiades Armenakas and Eleanor Rieffel},
   month = {3},
   note = {KeywordsGenerative modeling·Deep learning·Variational autoencoder·Anomaly detection·Restricted Boltzmannmachine·Gibbs sampling·Quantum-assisted machine learning},
   title = {Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete Deep Generative Model},
   url = {http://arxiv.org/abs/2303.12302},
   year = {2023},
}
@article{Wang2024,
   abstract = {Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together with a carefully designed "target attention" mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction. Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods. This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models.},
   author = {Zexin Wang and Changhua Pei and Minghua Ma and Xin Wang and Zhihan Li and Dan Pei and Saravan Rajmohan and Dongmei Zhang and Qingwei Lin and Haiming Zhang and Jianhui Li and Gaogang Xie},
   month = {2},
   note = {KEYWORDSUnivariate time series !!!!!!!!!!!!!!!!!!!!!!!!!!!!!, Anomaly detection, Conditional variationalautoencoder, Frequency information},
   title = {Revisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective},
   url = {http://arxiv.org/abs/2402.02820},
   year = {2024},
}
@article{Nguyen2024,
   abstract = {This paper aims to conduct a comparative analysis of contemporary Variational Autoencoder (VAE) architectures employed in anomaly detection, elucidating their performance and behavioral characteristics within this specific task. The architectural configurations under consideration encompass the original VAE baseline, the VAE with a Gaussian Random Field prior (VAE-GRF), and the VAE incorporating a vision transformer (ViT-VAE). The findings reveal that ViT-VAE exhibits exemplary performance across various scenarios, whereas VAE-GRF may necessitate more intricate hyperparameter tuning to attain its optimal performance state. Additionally, to mitigate the propensity for over-reliance on results derived from the widely used MVTec dataset, this paper leverages the recently-public MiAD dataset for benchmarking. This deliberate inclusion seeks to enhance result competitiveness by alleviating the impact of domain-specific models tailored exclusively for MVTec, thereby contributing to a more robust evaluation framework. Codes is available at https://github.com/endtheme123/VAE-compare.git.},
   author = {Huy Hoang Nguyen and Cuong Nhat Nguyen and Xuan Tung Dao and Quoc Trung Duong and Dzung Pham Thi Kim and Minh-Tan Pham},
   month = {8},
   note = {Index terms: computer vision (so more about images)},
   title = {Variational Autoencoder for Anomaly Detection: A Comparative Study},
   url = {http://arxiv.org/abs/2408.13561},
   year = {2024},
}
@article{Fayad2024,
   abstract = {Gravitational waves (GW), predicted by Einstein's General Theory of Relativity, provide a powerful probe of astrophysical phenomena and fundamental physics. In this work, we propose an unsupervised anomaly detection method using variational autoencoders (VAEs) to analyze GW time-series data. By training on noise-only data, the VAE accurately reconstructs noise inputs while failing to reconstruct anomalies, such as GW signals, which results in measurable spikes in the reconstruction error. The method was applied to data from the LIGO H1 and L1 detectors. Evaluation on testing datasets containing both noise and GW events demonstrated reliable detection, achieving an area under the ROC curve (AUC) of 0.89. This study introduces VAEs as a robust, unsupervised approach for identifying anomalies in GW data, which offers a scalable framework for detecting known and potentially new phenomena in physics.},
   author = {Ammar Fayad},
   month = {11},
   note = {keywords/},
   title = {Unsupervised Learning Approach to Anomaly Detection in Gravitational Wave Data},
   url = {http://arxiv.org/abs/2411.19450},
   year = {2024},
}
@article{Berahmand2024,
   abstract = {Autoencoders have become a hot researched topic in unsupervised learning due to their ability to learn data features and act as a dimensionality reduction method. With rapid evolution of autoencoder methods, there has yet to be a complete study that provides a full autoencoders roadmap for both stimulating technical improvements and orienting research newbies to autoencoders. In this paper, we present a comprehensive survey of autoencoders, starting with an explanation of the principle of conventional autoencoder and their primary development process. We then provide a taxonomy of autoencoders based on their structures and principles and thoroughly analyze and discuss the related models. Furthermore, we review the applications of autoencoders in various fields, including machine vision, natural language processing, complex network, recommender system, speech process, anomaly detection, and others. Lastly, we summarize the limitations of current autoencoder algorithms and discuss the future directions of the field.},
   author = {Kamal Berahmand and Fatemeh Daneshfar and Elaheh Sadat Salehi and Yuefeng Li and Yue Xu},
   doi = {10.1007/s10462-023-10662-6},
   issn = {15737462},
   issue = {2},
   journal = {Artificial Intelligence Review},
   keywords = {Autoencoder,Autoencoder application,Bottleneck layer,Deep learning,Dimensionality reduction,Feature extraction,Reconstruction loss,Unsupervised learning},
   month = {2},
   note = {Keywords  Deep learning · Dimensionality reduction · Feature extraction · Unsupervised learning · Autoencoder · Bottleneck layer · Reconstruction loss · Autoencoder application<br/><br/><br/>Useful for: <br/>* background of autoencoders (vanilla AE, hyperparameters in AE, objective functions, optimization algo..... ) <br/>* short paragraph on anomaly detection (supervised, unsupervised, semi-supervised)},
   publisher = {Springer Nature},
   title = {Autoencoders and their applications in machine learning: a survey},
   volume = {57},
   year = {2024},
}
@article{Wu2024,
   abstract = {Due to their unsupervised training and uncertainty estimation, deep Variational Autoencoders (VAEs) have become powerful tools for reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based TSAD methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data. However, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detection tasks. Such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces. We propose a novel generative framework that combines VAEs with self-supervised learning (SSL) to address this issue.},
   author = {Zhangkai Wu and Longbing Cao and Qi Zhang and Junxian Zhou and Hui Chen},
   month = {1},
   note = {Index Terms—Variational  Autoencoder,  Time  Series  AnomalyDetection,  Self  Supervised  Learning,  Data  Augmentation,  Con-trast  Learning,  Adversarial  Learning.},
   title = {Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection},
   url = {http://arxiv.org/abs/2401.03341},
   year = {2024},
}
@article{Neloy2024,
   author = {Asif Ahmed Neloy and Maxime Turgeon},
   doi = {10.1016/j.mlwa.2024.100572},
   issn = {26668270},
   journal = {Machine Learning with Applications},
   month = {9},
   note = {"However, in thisresearch, we narrow our focus to detecting anomalies specifically inimage data using novel AE techniques."},
   pages = {100572},
   publisher = {Elsevier BV},
   title = {A comprehensive study of auto-encoders for anomaly detection: Efficiency and trade-offs},
   volume = {17},
   year = {2024},
}
