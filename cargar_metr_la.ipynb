{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5576bbd1",
   "metadata": {},
   "source": [
    "# Urban Traffic Anomaly Detection using Variational Autoencoders (VAE)\n",
    "\n",
    "In this project, we aim to detect anomalies in urban traffic patterns using a probabilistic machine learning approach based on Variational Autoencoders (VAE). \n",
    "\n",
    "We will train the VAE on typical traffic data so that it learns common patterns. Then, we will detect anomalies as deviations from these learned patterns.\n",
    "\n",
    "The dataset used is the METR-LA dataset, which contains traffic speed readings from sensors in Los Angeles collected every 5 minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f747c2",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We will use Python 3.8+ and the following key libraries:\n",
    "\n",
    "- `h5py`: to read `.h5` dataset files  \n",
    "- `tables` (PyTables): a dependency needed by `pandas` to handle HDF5 files  \n",
    "- `numpy` and `pandas`: for data manipulation  \n",
    "- `matplotlib`: for visualization  \n",
    "- `torch` (PyTorch): to build and train our VAE model  \n",
    "\n",
    "### Installing required packages\n",
    "\n",
    "You can install them via pip:\n",
    "\n",
    "```bash\n",
    "pip install numpy pandas matplotlib h5py tables torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4a3e5",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "\n",
    "**METR-LA Dataset**\n",
    "\n",
    "- Contains traffic speed data from 207 sensors in Los Angeles.\n",
    "- Data is recorded every 5 minutes, resulting in 12 records per hour.\n",
    "- The data is stored in an HDF5 file format (`metr-la.h5`), where rows represent timestamps and columns correspond to different sensors.\n",
    "- Additionally, a precomputed sensor graph adjacency matrix is provided (`adj_mx.pkl`) which encodes the spatial relations between sensors.\n",
    "\n",
    "The data shape is approximately (34272, 207), meaning 34,272 time steps and 207 sensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ec621",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We will load the data from the `.h5` file using `h5py`, normalize the data, and create sliding windows of size 12 (1 hour of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890430d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34272, 207)\n",
      "<bound method NDFrame.head of                         773869     767541     767542     717447     717446  \\\n",
      "2012-03-01 00:00:00  64.375000  67.625000  67.125000  61.500000  66.875000   \n",
      "2012-03-01 00:05:00  62.666667  68.555556  65.444444  62.444444  64.444444   \n",
      "2012-03-01 00:10:00  64.000000  63.750000  60.000000  59.000000  66.500000   \n",
      "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "...                        ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  65.000000  65.888889  68.555556  61.666667   0.000000   \n",
      "2012-06-27 23:40:00  61.375000  65.625000  66.500000  62.750000   0.000000   \n",
      "2012-06-27 23:45:00  67.000000  59.666667  69.555556  61.000000   0.000000   \n",
      "2012-06-27 23:50:00  66.750000  62.250000  66.000000  59.625000   0.000000   \n",
      "2012-06-27 23:55:00  65.111111  66.888889  66.777778  61.222222   0.000000   \n",
      "\n",
      "                        717445     773062     767620     737529     717816  \\\n",
      "2012-03-01 00:00:00  68.750000  65.125000  67.125000  59.625000  62.750000   \n",
      "2012-03-01 00:05:00  68.111111  65.000000  65.000000  57.444444  63.333333   \n",
      "2012-03-01 00:10:00  66.250000  64.500000  64.250000  63.875000  65.375000   \n",
      "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "...                        ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  54.555556  62.444444  63.333333  59.222222  65.333333   \n",
      "2012-06-27 23:40:00  50.500000  62.000000  67.000000  65.250000  67.125000   \n",
      "2012-06-27 23:45:00  44.777778  64.222222  63.777778  59.777778  57.666667   \n",
      "2012-06-27 23:50:00  53.000000  64.285714  64.125000  60.875000  66.250000   \n",
      "2012-06-27 23:55:00  49.555556  65.777778  65.111111  63.000000  61.666667   \n",
      "\n",
      "                     ...     772167     769372     774204     769806  \\\n",
      "2012-03-01 00:00:00  ...  45.625000  65.500000  64.500000  66.428571   \n",
      "2012-03-01 00:05:00  ...  50.666667  69.875000  66.666667  58.555556   \n",
      "2012-03-01 00:10:00  ...  44.125000  69.000000  56.500000  59.250000   \n",
      "2012-03-01 00:15:00  ...   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00  ...   0.000000   0.000000   0.000000   0.000000   \n",
      "...                  ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  ...  52.888889  69.000000  65.111111  55.666667   \n",
      "2012-06-27 23:40:00  ...  54.000000  69.250000  60.125000  60.500000   \n",
      "2012-06-27 23:45:00  ...  51.333333  67.888889  64.333333  57.000000   \n",
      "2012-06-27 23:50:00  ...  51.125000  69.375000  61.625000  60.500000   \n",
      "2012-06-27 23:55:00  ...  56.000000  67.444444  64.888889  60.888889   \n",
      "\n",
      "                        717590     717592     717595     772168     718141  \\\n",
      "2012-03-01 00:00:00  66.875000  59.375000  69.000000  59.250000  69.000000   \n",
      "2012-03-01 00:05:00  62.000000  61.111111  64.444444  55.888889  68.444444   \n",
      "2012-03-01 00:10:00  68.125000  62.500000  65.625000  61.375000  69.857143   \n",
      "2012-03-01 00:15:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "2012-03-01 00:20:00   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "...                        ...        ...        ...        ...        ...   \n",
      "2012-06-27 23:35:00  66.333333  62.444444  66.777778  64.888889  69.666667   \n",
      "2012-06-27 23:40:00  67.250000  59.375000  66.000000  61.250000  69.000000   \n",
      "2012-06-27 23:45:00  66.000000  62.666667  68.666667  63.333333  67.444444   \n",
      "2012-06-27 23:50:00  65.625000  66.375000  69.500000  63.000000  67.875000   \n",
      "2012-06-27 23:55:00  64.222222  66.444444  68.444444  63.555556  68.666667   \n",
      "\n",
      "                        769373  \n",
      "2012-03-01 00:00:00  61.875000  \n",
      "2012-03-01 00:05:00  62.875000  \n",
      "2012-03-01 00:10:00  62.000000  \n",
      "2012-03-01 00:15:00   0.000000  \n",
      "2012-03-01 00:20:00   0.000000  \n",
      "...                        ...  \n",
      "2012-06-27 23:35:00  62.333333  \n",
      "2012-06-27 23:40:00  62.000000  \n",
      "2012-06-27 23:45:00  61.222222  \n",
      "2012-06-27 23:50:00  63.500000  \n",
      "2012-06-27 23:55:00  61.777778  \n",
      "\n",
      "[34272 rows x 207 columns]>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ruta al archivo .h5\n",
    "file_path = 'data/metr-la.h5'\n",
    "df = pd.read_hdf(file_path)\n",
    "print(df.shape)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9323f4",
   "metadata": {},
   "source": [
    "#### Data Normalization\n",
    "\n",
    "We use `StandardScaler` from scikit-learn to apply Z-score normalization to the dataset.\n",
    "\n",
    "Each sensor (column) is standardized independently by removing the mean and scaling to unit variance:\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320090e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(df.values)  # shape: (34272, 207)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e8288",
   "metadata": {},
   "source": [
    "#### Sliding Windows\n",
    "\n",
    "Sliding windows allow us to structure the time series data as sequences of fixed length, suitable for feeding into our VAE model.\n",
    "\n",
    "The window size of 12 means each sample contains 12 consecutive timesteps (i.e., one hour of data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34260, 12, 207)\n"
     ]
    }
   ],
   "source": [
    "def create_sliding_windows(data, window_size):\n",
    "    X = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "    return np.array(X)\n",
    "\n",
    "window_size = 12  # 12 pasos = 1 hora\n",
    "X = create_sliding_windows(data_normalized, window_size)  # shape: (34260, 12, 207)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c83a5",
   "metadata": {},
   "source": [
    "#### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a79140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (23982, 12, 207)\n",
      "Validation shape: (5139, 12, 207)\n",
      "Test shape: (5139, 12, 207)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X.shape = (34260, 12, 207)\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "# Definimos proporciones\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculamos índices\n",
    "train_end = int(num_samples * train_ratio)\n",
    "val_end = train_end + int(num_samples * val_ratio)\n",
    "\n",
    "# División\n",
    "X_train = X[:train_end]\n",
    "X_val = X[train_end:val_end]\n",
    "X_test = X[val_end:]\n",
    "\n",
    "print(f'Train shape: {X_train.shape}')\n",
    "print(f'Validation shape: {X_val.shape}')\n",
    "print(f'Test shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c984c",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "\n",
    "A Variational Autoencoder (VAE) is a type of generative model that learns to encode input data into a compressed latent space and then reconstruct it as accurately as possible. Unlike traditional autoencoders, VAEs add a probabilistic framework that makes the latent space continuous and meaningful, which is useful for tasks like anomaly detection.\n",
    "\n",
    "### Main Components\n",
    "\n",
    "#### 1. **Encoder**\n",
    "The encoder network takes an input (e.g., a time window of traffic data) and maps it to a latent distribution. Instead of directly outputting a single latent vector, the encoder outputs:\n",
    "\n",
    "- A mean vector $\\mu$\n",
    "- A standard deviation (or log variance) vector $\\log(\\sigma^2)$\n",
    "\n",
    "These define a multivariate normal distribution from which we sample the latent variable $z$.\n",
    "\n",
    "#### 2. **Latent Sampling**\n",
    "To allow backpropagation through the sampling process, VAEs use the **reparameterization trick**:\n",
    "\n",
    "$$\n",
    "z = \\mu + \\sigma \\cdot \\epsilon \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "This makes the sampling step differentiable.\n",
    "\n",
    "#### 3. **Decoder**\n",
    "The decoder takes the sampled latent variable $z$ and tries to reconstruct the original input data. The goal is to learn a meaningful latent space that can generate realistic reconstructions.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The VAE optimizes two components in the loss:\n",
    "\n",
    "1. **Reconstruction Loss** (e.g., Mean Squared Error):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{recon}} = ||x - \\hat{x}||^2\n",
    "$$\n",
    "\n",
    "This penalizes the model when its reconstruction $\\hat{x}$ is far from the original input $x$.\n",
    "\n",
    "> **Why use MSE as reconstruction loss?**\n",
    ">\n",
    "> Theoretically, we want to maximize the log-likelihood of the reconstruction, i.e., $\\log p(x|z)$. If we assume that the decoder’s output is Gaussian with fixed variance (i.e., $p(x|z) = \\mathcal{N}(\\hat{x}, \\sigma^2 I)$), then:\n",
    ">\n",
    "> $$\n",
    "> \\log p(x|z) = -\\frac{1}{2\\sigma^2} ||x - \\hat{x}||^2 + \\text{const}\n",
    "> $$\n",
    ">\n",
    "> So minimizing the squared error (MSE) is equivalent to maximizing the likelihood. That’s why MSE acts as a **proxy** for the negative log-likelihood in practice.\n",
    "\n",
    "\n",
    "2. **Kullback-Leibler (KL) Divergence**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{KL}} = D_{\\text{KL}}(q(z|x) || p(z))\n",
    "$$\n",
    "\n",
    "This measures how much the learned latent distribution $q(z|x)$ (given by the encoder) deviates from the standard normal prior $p(z) = \\mathcal{N}(0, I)$.\n",
    "\n",
    "> **In practice**, the encoder outputs $\\mu$ and $\\log(\\sigma^2)$, and the KL divergence between the approximate posterior and the prior can be computed in closed form:\n",
    ">\n",
    "> $$\n",
    "> \\mathcal{L}_{\\text{KL}} = -\\frac{1}{2} \\sum \\left(1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2\\right)\n",
    "> $$\n",
    ">\n",
    "> This encourages the model to keep the latent representations close to a standard normal distribution.\n",
    "\n",
    "\n",
    "### Total Loss\n",
    "\n",
    "The total loss combines both terms:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{recon}} + \\beta \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "where $\\beta$ is a hyperparameter (commonly set to 1) that balances reconstruction accuracy and regularization. \n",
    "\n",
    "> **Important:** In theory, we want to **maximize** the Evidence Lower Bound (ELBO), which is:\n",
    ">\n",
    "> $$\n",
    "> \\text{ELBO}(x) = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{\\text{KL}}(q(z|x) || p(z))\n",
    "> $$\n",
    ">\n",
    "> But in practice, optimization libraries **minimize** functions. So minimizing the total loss is equivalent to **maximizing the ELBO**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
